[
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.4-api-gateway/5.4.1-create-api/",
	"title": "Create REST API",
	"tags": [],
	"description": "",
	"content": "Create API Gateway (REST) Open API Gateway -\u0026gt; Create API -\u0026gt; REST API -\u0026gt; Build. Name: serverless-workshop-api; Endpoint type: Regional -\u0026gt; Create API. In Resources: select / -\u0026gt; Create resource -\u0026gt; Path: hello -\u0026gt; Create. (Optional) Enable CORS on the resource if you plan to call from a browser. Image placeholder: REST API + /hello resource creation\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/4-eventparticipated/4.1-event1/",
	"title": "Event 1 - Vietnam Cloud Day 2025",
	"tags": [],
	"description": "",
	"content": "Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders Date \u0026amp; Time: Thursday, 18 September 2025 | 9:00 – 17:00 VNT\nLocation: Amazon Web Services Vietnam, 36th Floor, 2 Hai Trieu Street, Sai Gon Ward, Ho Chi Minh City\nRegistration Status: Closed\nEvent Overview Vietnam Cloud Day 2025 was a comprehensive AWS event designed for builders and enterprise leaders, featuring keynote addresses from government speakers, AWS executives, and industry leaders. The event showcased AWS\u0026rsquo;s latest services and strategic initiatives for AI and cloud modernization through two main tracks: a live telecast track and in-person breakout sessions.\nAgenda Live Telecast Track Time (VNT) Session Speaker 7:35 - 9:00 Registration - 9:00 - 9:20 Opening Hon. Government Speaker 9:20 - 9:40 Keynote Address Eric Yeo, Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS 9:40 - 10:00 Customer Keynote 1 Dr. Jens Lottner, CEO, Techcombank 10:00 - 10:20 Customer Keynote 2 Ms. Trang Phung, CEO \u0026amp; Co-Founder, U2U Network 10:20 - 10:50 AWS Keynote Jaime Valles, Vice President, General Manager Asia Pacific and Japan, AWS 11:00 – 11:40 Panel Discussion: Navigating the GenAI Revolution Moderator: Jeff Johnson, Managing Director, ASEAN, AWS Panel Discussion Details: Navigating the GenAI Revolution: Strategies for Executive Leadership This discussion delved into how executive leaders can effectively steer their organizations through the rapid advancements in generative AI. Executive panelists shared their insights and personal journeys on:\nFostering a culture of innovation Aligning AI initiatives with business objectives Managing organizational changes that accompany AI integration Panelists:\nVu Van, Co-founder \u0026amp; CEO, ELSA Corp Nguyen Hoa Binh, Chairman, Nexttech Group Dieter Botha, CEO, TymeX Breakout Tracks (In-Person Sessions) Track 1: AI \u0026amp; Analytics Focus Time (VNT) Session Speaker 13:15 - 13:30 Opening \u0026amp; Track Introduction Jun Kai Loke, AI/ML Specialist SA, AWS 13:30 - 14:00 Building a Unified Data Foundation on AWS for AI and Analytics Workloads Kien Nguyen, Solutions Architect, AWS 14:00 - 14:30 Building the Future: Gen AI Adoption and Roadmap on AWS Jun Kai Loke, AI/ML Specialist SA, AWS; Tamelly Lim, Storage Specialist SA, AWS 14:30 - 15:00 AI-Driven Development Lifecycle (AI-DLC) - Shaping the Future of Software Implementation Binh Tran, Senior Solutions Architect, AWS 15:00 - 15:30 Tea Break - 15:30 - 16:00 Securing Generative AI Applications with AWS: Fundamentals and Best Practices Taiki Dang, Solutions Architect, AWS 16:00 - 16:30 Beyond Automation: AI Agents as Your Ultimate Productivity Multipliers Michael Armentano, Principal WW GTM Specialist, AWS Session Details Building a Unified Data Foundation on AWS for AI and Analytics Workloads\nThis session delved into strategies and best practices for constructing a unified, scalable data foundation on AWS. Participants learned how to leverage AWS services to create a robust data infrastructure that can handle the demands of modern data-driven applications. Key topics covered:\nData ingestion, storage, processing, and governance Effective data management and utilization for advanced analytics and AI initiatives Building the Future: Gen AI Adoption and Roadmap on AWS\nAWS presented its comprehensive vision, emerging trends, and strategic roadmap for the adoption of Generative AI (GenAI) technologies. The discussion covered key AWS services and initiatives designed to empower organizations in leveraging GenAI to drive innovation and efficiency.\nAI-Driven Development Lifecycle (AI-DLC) - Shaping the Future of Software Implementation\nThe AI-Driven Development Lifecycle (AI-DLC) is a transformative, AI-centric approach reshaping the future of software implementation by fully embedding AI as a central collaborator in the entire software development lifecycle. Unlike traditional methods that retrofit AI as an assistant to existing human-driven processes, AI-DLC integrates AI-powered execution with human oversight and dynamic team collaboration to:\nDrastically improve software development speed Enhance code quality Foster innovation Securing Generative AI Applications with AWS: Fundamentals and Best Practices\nThis session explored the unique security challenges at each layer of the generative AI stack—infrastructure, models, and applications. Participants learned how AWS integrates built-in security measures such as:\nEncryption Zero-trust architecture Continuous monitoring Fine-grained access controls These measures safeguard generative AI workloads, ensuring data confidentiality and integrity throughout the AI lifecycle.\nBeyond Automation: AI Agents as Your Ultimate Productivity Multipliers\nThis session presented a paradigm shift where AI agents aren\u0026rsquo;t just tools, but intelligent partners actively driving business forward. Key concepts included:\nAI agents that learn, adapt, and execute complex tasks autonomously Transformation of operations from manual processes to unprecedented efficiency Exponential productivity multiplication through AI power Track 2: Cloud Migration \u0026amp; Modernization Focus Time (VNT) Session Speaker 13:15 - 13:30 Opening \u0026amp; Track Introduction Hung Nguyen Gia, Head of Solutions Architect, AWS 13:30 - 14:00 Completing a Large-Scale Migration and Modernisation with AWS Son Do, Technical Account Manager, AWS; Nguyen Van Hai, Director of Software Engineering, Techcombank 14:00 - 14:30 Modernizing Applications with Generative AI-Powered Tools Phuc Nguyen, Solutions Architect, AWS; Alex Tran, AI Director, OCB 14:30 - 15:00 Panel Discussion: Application Modernization - Accelerating Business Transformation Moderator: Hung Nguyen Gia, Head of Solutions Architect, AWS 15:00 - 15:30 Break - 15:30 - 16:00 Transforming VMware with AI-driven Cloud Modernisation Hung Hoang, Customer Solutions Manager, AWS 16:00 - 16:30 AWS Security at Scale: From Development to Production Taiki Dang, Solutions Architect, AWS Session Details Completing a Large-Scale Migration and Modernisation with AWS\nThis session focused on valuable lessons from thousands of enterprises who have migrated and modernized their on-premises workloads with AWS. Topics included:\nProven mental models and technical best practices Modernization pathways that help organizations modernize while they migrate AWS migration accelerators and latest migration and modernization tools Case study showcasing how organizations established a robust foundation and strategic roadmap leveraging AWS cloud capabilities to achieve digital transformation goals Modernizing Applications with Generative AI-Powered Tools\nThis session explored how Amazon Q Developer transforms the software development lifecycle (SDLC) through its agentic capabilities across:\nAWS Console IDE CLI DevSecOps platforms Key capabilities demonstrated:\nQ\u0026rsquo;s agents accelerate code generation and improve code quality Seamless integration with existing workflows Automatic generation of comprehensive documentation and unit tests Improved code maintainability and reliability Understanding of complex codebases and optimization suggestions Automation of routine tasks across the development lifecycle Panel Discussion: Application Modernization - Accelerating Business Transformation\nPanelists:\nNguyen Minh Ngan, AI Specialist, OCB Nguyen Manh Tuyen, Head of Data Application, LPBank Securities Vinh Nguyen, Co-Founder \u0026amp; CTO, Ninety Eight Transforming VMware with AI-driven Cloud Modernisation\nThis session showed how Vietnamese organizations are accelerating cloud adoption with VMware estates. Key topics:\nHow AWS Transform helps migrate fast, safely, and cost-effectively Step-by-step playbook and downtime-aware patterns Roadmap to modernize onto EKS, RDS, and serverless after landing Ideal for IT leaders, architects, and ops teams planning large-scale VMware-to-AWS moves AWS Security at Scale: From Development to Production\nThis session explored how to enhance cloud security posture across the entire development and production lifecycle. Topics covered:\nAWS comprehensive security approach: identification, prevention, detection, response, and remediation Security-by-design principles throughout the development process Advanced detection and response capabilities How generative AI enhances security analysis and automates operations Building resilient architectures that evolve with emerging threats Creating more secure, scalable cloud environments Key Takeaways Comprehensive understanding of AWS\u0026rsquo;s AI and cloud modernization strategy Practical insights into enterprise-scale AI adoption and implementation Best practices for data foundation, security, and application modernization Real-world case studies and lessons from industry leaders Hands-on knowledge of AWS services for GenAI, migration, and modernization "
},
{
	"uri": "http://localhost:1313/hugo_aws/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Bùi Văn Vũ\nPhone Number: 0353380809\nEmail: vubui.cv2021@gmail.com\nUniversity: FPT University\nMajor: Artificial Intelligence\nId: SE180080\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 8/09/2025 to 8/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Goals Introduce the serverless workshop: create Lambda (Node.js/Python), accept input parameters, integrate API Gateway, and deploy an in-app purchase suggestion API. Emphasize fast delivery with Lambda without managing servers.\nArchitecture at a glance Lambda receives events, processes logic, and returns JSON. API Gateway is the REST front door mapping methods to Lambda. Optional: custom domain/stage for multiple environments. Expected outcomes Two Hello World functions (Node.js/Python) with parameters. One REST endpoint (GET/POST) via API Gateway. One business-logic function (purchase suggestions) returning JSON. Timing \u0026amp; structure Part 1: Lambda basics, testing in the console. Part 2: API Gateway, input mapping, testing. Part 3: In-app purchase exercise, clean up resources. "
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.3-lambda-basics/5.3.1-hello-node/",
	"title": "Lambda Hello World (Node.js)",
	"tags": [],
	"description": "",
	"content": "Create the Hello World function (Node.js) Open Lambda console -\u0026gt; Create function -\u0026gt; Author from scratch. Name: hello-node, Runtime: Node.js 18.x, Role: AWSLambdaBasicExecutionRole. Paste the sample handler: exports.handler = async (event) =\u0026gt; { const name = (event?.queryStringParameters || {}).name || event?.name || \u0026#34;world\u0026#34;; return { statusCode: 200, headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; }, body: JSON.stringify({ message: `Hello, ${name}` }) }; }; Save. Test in the console Click Test -\u0026gt; create a new event, body: {\u0026quot;name\u0026quot;:\u0026quot;Alice\u0026quot;}. Run Test and check the response Hello, Alice. Open Monitor -\u0026gt; Logs to view the CloudWatch log stream. Basic configuration Memory/Timeout: increase/decrease as needed (e.g., 128�256 MB, 3�10s). Log retention: open CloudWatch Log group /aws/lambda/hello-node -\u0026gt; Edit retention (7�14 days). If name is missing, you can return 400 with a clear message (optional). "
},
{
	"uri": "http://localhost:1313/hugo_aws/3-blogstranslated/3.1-blog1/",
	"title": "Strategies to excel in all four exam domains of the AWS Certified Machine Learning – Specialty certification.",
	"tags": [],
	"description": "",
	"content": "Strategies to excel in all four exam domains of the AWS Certified Machine Learning – Specialty certification.\nBy Isha Doshi and Aizhamal Nazhimidinova | on August 26, 2025 | in Amazon Machine Learning, Amazon SageMaker, AWS Training and Certification | Permalink | Share\nAs organizations rapidly adopt AI solutions, the demand for machine learning (ML) experts continues to grow. According to the World Economic Forum Future of Jobs Report 2025, the demand for AI and ML professionals is expected to increase by more than 80% by 2030.\nTo meet this growing demand, Amazon Web Services (AWS) offers a comprehensive certification path that helps professionals build and validate ML knowledge and skills in building, training, tuning, and deploying ML models.\nIn this article, we explain how to prepare for the AWS Certified Machine Learning – Specialty, whether you are starting from scratch or building on existing AWS certifications. We share the prerequisites and instructions to help you prepare for this certification and demonstrate your expertise in building ML solutions with AWS.\nFour Domains: Comprehensive Study Guide\nThe AWS Certified Machine Learning – Specialty Exam Guide provides a blueprint of the certification structure, detailing the four key domains and specific tasks. This document serves as an official roadmap for candidates, outlining the key knowledge and skills required to demonstrate ML competency with AWS.\nThe four domains are as follows:\nData engineering (20% of the score)\nExploratory data analysis (24% of the score)\nModeling (36% of the score)\nMachine learning implementation and operations (20% of the score)\n1: Data engineering\nThe data engineering domain focuses on the critical skills in managing and transforming data for ML processes. Candidates must demonstrate competency in creating data repositories, identifying data sources, and implementing ingestion solutions using AWS services. This domain covers essential data transformation skills, including ETL processes and data pipelines required to develop effective machine learning solutions.\nAWS Skill Builder Exam Prep Plan: AWS Certified Machine Learning – Specialty provides you with domain-specific study materials along with official practice question sets. The Domain 1 Review: AWS Certified Machine Learning – Specialty course provides expert-led video instruction that links AWS services to key learning objectives. To learn more, you can explore the Data Engineer course, which covers Domain 1 concepts and AWS services.\n2: Exploratory data analysis\nThe exploratory data analysis domain focuses on the key skills needed to transform raw data into ML-ready insights. Candidates must demonstrate proficiency in data preparation, feature engineering, and techniques to uncover hidden patterns in datasets. This domain assesses your readiness to handle data preprocessing, normalization, and feature selection – essential elements for improving ML model performance.\nThe course Domain 2 Review: AWS Certified Machine Learning – Specialty provides expert-led video instruction, linking to the AWS services with key learning objectives on data analysis. The course Digital Classroom – Practical Data Science with Amazon SageMaker has modules and labs covering data preparation and transformation techniques.\n3: Modeling\nModeling is the largest part of the AWS Certified Machine Learning – Specialty certification, covering the entire modeling lifecycle across multiple learning paradigms. Candidates must demonstrate an understanding of ML algorithms, model training techniques, and evaluation metrics. This area challenges professionals to master critical skills in algorithm selection, model training, and performance evaluation in a variety of ML scenarios.\nThe course Domain 3 Review: AWS Certified Machine Learning – Specialty provides expert-led video instruction that connects AWS services to key ML modeling techniques. Amazon SageMaker AI Getting Started is a complementary resource for hands-on learning.\n4: Machine learning implementation and operations\nSection 4 focuses on transforming ML models into production-ready solutions. Candidates must demonstrate expertise in deployment strategies, ML operations lifecycle management (MLOps), and model monitoring. This field challenges professionals to master key skills in deploying ML solutions, optimizing infrastructure, and ensuring effective model performance in production environments.\nThe course Domain 4 Review: AWS Certified Machine Learning – Specialty provides expert-led video tutorials that link AWS services to key MLOps and deployment strategies. To learn more about the deployment and operational aspects of ML, check out the course Digital Classroom – MLOps Engineering on AWS.\nRoadmap to ML Specialty Certification\nStarting your journey to successfully completing the AWS Certified Machine Learning – Specialty certification requires a solid foundation. You should have a basic understanding of Python programming and be familiar with basic statistical concepts and ML principles. Becoming an AWS ML specialist can follow a structured learning path, but the certification can be achieved through any learning path that helps you build the necessary expertise.\nFor candidates who want to follow a structured path, here are the steps to take:\nStep 1: AWS Certified AI Practitioner (CLF-C02)\nPerfect for beginners and business professionals, this entry-level certification focuses on practical AI knowledge, foundational concepts, and introduces core AWS AI tools such as Amazon SageMaker, Amazon Comprehend, and Amazon Lex.\nStep 2: AWS Certified Machine Learning Engineer – Associate (MLA-C01)\nThis intermediate-level certification focuses on the entire ML lifecycle and is designed for those who directly implement, deploy into production, and maintain ML solutions on AWS. It covers everything from data preparation and model training to workflow orchestration and monitoring.\nStep 3: AWS Certified Machine Learning – Specialty (MLS-C01)\nFor experienced professionals with at least 2 years of work in the ML field, this advanced certification validates expertise in data engineering, analytics, and model optimization.\nThis path helps you develop both breadth and depth in your cloud computing knowledge and build specialized expertise in ML technologies—skills that are increasingly sought after as organizations continue to leverage AI/ML for competitive advantage.\nFor more information on building your AI/ML career path, including preparation resources and strategic direction, visit the Mapping your AI/ML career journey article in the AWS Training and Certification Blog.\nBuilding from an AWS Certified Machine Learning Engineer – Associate or AWS Data Engineer – Associate\nWhether you come from a data engineering or ML background, Even ML Specialty requires mastering the entire ML lifecycle. Both paths converge at the heart of the ML Specialty in the following areas:\nEnd-to-end ML pipelines – Understand how data is processed from ingestion to preprocessing, training, evaluation, and deployment\nProduction-grade ML systems – Build scalable and secure ML solutions\nAdvanced feature engineering – Create features that improve model performance\nMLOps practices – Implement continuous integration and deployment (CI/CD) for ML models\nPrepare for the ML Specialty with a foundation in data engineering\nAs a certified AWS Data Engineer – Associate, your expertise in services such as AWS Glue, Amazon EMR, and data storage options provides a solid foundation for the Data Engineering and Exploratory Data Analysis areas of the AWS Certified Machine exam Learning – Specialty. To effectively bridge the gap, focus on expanding your knowledge of ML-specific data preparation techniques, including feature engineering, data cleaning for ML workloads, and understanding how data quality impacts model performance.\nPay special attention to SageMaker’s data preparation capabilities, which may be new to the data engineering toolset you’re already familiar with. Domain 3 will bring in the most current knowledge as you’ll need to develop expertise in algorithm selection, hyperparameter tuning, and model evaluation metrics—topics not covered in depth in the data engineering certification.\nPrepare for ML Specialty with a Foundation in ML Engineering\nAs an AWS Certified Machine Learning Engineer – Associate, you’re already familiar with SageMaker and the fundamentals of building, training, and deploying ML models. Your strengths in modeling give you an early advantage in most of the ML Specialty exam. However, to excel in the ML Specialty certification, you will need to deepen your understanding of data engineering aspects that support complex ML workflows.\nFocus on expanding your knowledge of large-scale data processing systems like Amazon EMR and AWS Glue, which are not emphasized in the associate certification. The ML Specialty exam requires a more advanced understanding of exploratory data analysis, including statistical methods and visualization techniques to discover patterns in large data sets. It requires expertise in optimizing models for production environments and deploying complex MLOps. You should also strengthen your knowledge of operational aspects like model monitoring and deploying ML pipelines at scale.\nNext Steps on Your Journey\nAWS offers a variety of training options designed to accommodate different learning styles, including Exam Prep Plan, hands-on labs, and [interactive games](https://skillbuilder.aws/search?sc_icampaign=aware_global_110_digitaltraining_evergreen_2024_sbis_midpage_tnc\u0026sc_ichannel=ha\u0026sc_icontent=awssm-4892_aware_tnc\u0026sc_iplace= 1up\u0026amp;sc_channel=ha\u0026amp;trk=47117ee4-722a-4ab2-bbcb-3742ea6e8a96~ha_awssm-4892_aware_tnc\u0026amp;searchText=machine-learning-learning-plan-includes-labs\u0026amp;showRedirectNotFoundBanner=true),[AWS Training Live](https://aws.amazon.com/training/twitch/?ams%23interactive-card-vertical%23pattern-data.filter=%257B%2522filters%2522%253A%255B%255D%257D\u0026a ms%23interactive-card-vertical%23pattern-data-1258334469.filter=%257B%2522search%2522%253A%2522ML%2522%252C%2522filters%2522%253A%255B%255D%257D) for expert-led cloud training courses, along with [free online and in-person training events](https://aws.amazon.com/training/events/?ams%23interactive-card-vertical%23pattern-data.filter=%257B%2522filters%2522%253A%255B%255D%257D\u0026ams %23interactive-card-vertical%23pattern-data-1504449867.filter=%257B%2522search%2522%253A%2522ML%2520%2522%252C%2522filters%2522%253A%255B%255D%257D). By using resources like AWS Skill Builder, AWS Educate, and Udemy Business Leadership Academy, you can accelerate your learning in the rapidly evolving field of AI/ML.\nSkill Builder offers free Official Practice Question Sets to help you understand the exam format. These 20-question sets, developed by AWS, represent the style of the certification exam.\nSign up for courses on Skill Builder, complete the provided learning materialsmentioned in this article and schedule your exam today!\nSource:https://aws.amazon.com/vi/blogs/training-and-certification/strategies-for-excelling-across-all-four-exam-domains-of-the-aws-certified-machine-learning-specialty-certificatio n/\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.1-week1/",
	"title": "Week 1 - Cloud Computing Fundamentals",
	"tags": [],
	"description": "",
	"content": "Week: 2025-09-08 to 2025-09-12\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 1 Overview This week covered cloud computing fundamentals, AWS global infrastructure, and the core management tooling offered by AWS.\nKey Topics Introduction to Cloud Computing and its benefits AWS Global Infrastructure (Regions, AZs, Edge Locations) AWS Management Tools (Console, CLI, SDK) Cost Optimization approaches and Support Plans AWS Well-Architected Framework pillars Hands-on Labs Lab 01: AWS Account \u0026amp; IAM Setup Lab 07: AWS Budgets \u0026amp; Cost Management Lab 09: AWS Support Plans Day 1 -Clearly explain what Cloud Computing is: providing IT resources (such as computing power, storage, databases) on demand, usually over the internet, on a pay-as-you-go model. Core benefits:\n-Cost Savings: Shift from capital expenditure (CAPEX) to operating expenditure (OPEX).\n-Speed ​​and Agility: Deploy resources in minutes instead of weeks or months.\n-Global Scale: Easily scale applications internationally.\nDay 2- Learn about the massive physical structure that enables AWS to deliver reliable and high-performance services worldwide:\n-Region: A distinct physical geographic area where AWS groups its Data Centers. Each Region is completely isolated from other Regions to ensure high fault tolerance.\n-Availability Zone (AZ): One or more discrete Data Centers, with separate power, network and connectivity. Each Region has at least two AZs. AZs in the same Region are connected by low latency lines, allowing for building applications with high availability.\n-Edge Location: Smaller data centers, used primarily by Amazon CloudFront (content delivery network - CDN) to store content (cache) near end users, helping to reduce latency.\nDay 3-Introduce the main interfaces and tools for interacting and managing resources on AWS:\n-AWS Management Console: Web-based graphical user interface (GUI), easy to use, ideal for beginners and manual management tasks.\n-AWS Command Line Interface (CLI): Powerful command-line tools that enable task automation and service management through scripting.\n-AWS Software Development Kits (SDKs): Libraries that allow developers to integrate AWS services into their applications using popular programming languages ​​(e.g., Python, Java, Node.js).\nDay 4-Guide to managing and optimizing AWS spend to ensure the most efficient use of resources, including leveraging different payment models and cost management tools.\nDay 5-Introduce a set of guiding principles that help cloud architects design and operate secure, high-performance, resilient, and cost-effective systems on AWS. The framework is divided into five main pillars:\nOperational Excellence.\nSecurity.\nReliability.\nPerformance Efficiency.\nCost Optimization.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/",
	"title": "Worklog - AWS Learning Journey",
	"tags": [],
	"description": "",
	"content": "Worklog Overview 12-week worklog (5 days/week) starting September 8, 2025, summarizing study topics for evaluation: AWS foundations, architecture, and recent GenAI/compute releases from re:Invent.\nWeekly Theme Snapshot Week 1: Cloud fundamentals, global infra, Well-Architected Week 2: Networking (VPC, SG/NACL, load balancers, TGW/peering, VPN/DC) Week 3: Compute (EC2/AMI/EBS, scaling, pricing models, EFS/FSx) Week 4: Storage (S3 classes, Glacier, Snow, DR/Backup) Week 5: Security \u0026amp; Identity (IAM, Org, KMS, Security Hub, SSO) Week 6: Databases (RDS/Aurora/Redshift, ElastiCache, DMS) Week 7: Serverless \u0026amp; Containers (Lambda, ECS/EKS, ECR) Week 8: Monitoring \u0026amp; Ops (CloudWatch, X-Ray, CloudTrail, cost/ops) Week 9: Transformer architecture \u0026amp; implementation (attention, encoder-decoder, GPT/BERT/T5 overview) Week 10: Transfer learning, QA modes, BERT \u0026amp; T5 fine-tuning fundamentals Week 11: Lambda Managed Instances (capacity providers, networking, scaling, cost controls) Week 12: AWS re:Invent 2025 launches (Nova models, Bedrock updates, S3 Vectors, SageMaker serverless/elastic training, Graviton5, Trainium3) "
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.4-api-gateway/5.4.2-add-post/",
	"title": "Add POST Method",
	"tags": [],
	"description": "",
	"content": "Configure POST /hello Select the /hello resource → Add method → POST → Lambda Function. Choose the same Lambda function used for GET. Use Lambda proxy integration (recommended) so Lambda receives the full request; parse event.body (JSON) in the function. Deploy \u0026amp; Test Redeploy the dev stage. Test the endpoint: curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\\\ -d \u0026#39;{}\u0026#39; \\\\ \\\u0026#34;https://\u0026lt;id\u0026gt;.execute-api.\u0026lt;region\u0026gt;.amazonaws.com/dev/hello\\\u0026#34; Ensure the JSON response returns the correct name; check CloudWatch logs if there are errors. "
},
{
	"uri": "http://localhost:1313/hugo_aws/4-eventparticipated/4.2-event2/",
	"title": "Event 2 - AWS GenAI Builder Club: AI-Driven Development Life Cycle",
	"tags": [],
	"description": "",
	"content": "AWS GenAI Builder Club: AI-Driven Development Life Cycle - Reimagining Software Engineering Date \u0026amp; Time: Friday, October 3, 2025 | 14:00 (2:00 PM)\nLocation: AWS Event Hall, L26 Bitexco Tower, Ho Chi Minh City\nInstructors: Toan Huynh \u0026amp; My Nguyen\nCoordinators: Diem My, Dai Truong, Dinh Nguyen\nEvent Overview This AWS GenAI Builder Club session explored the AI-Driven Development Lifecycle (AI-DLC), a transformative approach to software engineering that integrates AI as a central collaborator throughout the entire development process. The session featured hands-on demonstrations of Amazon Q Developer and Kiro, showcasing practical applications of AI in modern software development.\nAgenda Time Session Instructor 14:00 - 14:15 Welcoming - 14:15 - 15:30 AI-Driven Development Life Cycle Overview \u0026amp; Amazon Q Developer Demonstration Toan Huynh 15:30 - 15:45 Break - 15:45 - 16:30 Kiro Demonstration My Nguyen Key Concepts \u0026amp; Learnings 1. AI-Driven Development Lifecycle (AI-DLC) Overview Core Philosophy The AI-Driven Development Lifecycle represents a fundamental shift in how software is built. Rather than treating AI as an afterthought or simple code completion tool, AI-DLC embeds AI as an intelligent partner throughout the entire development process.\nKey Principles:\nYou Are in Control - AI is your assistant, not your manager. You must maintain decision-making authority over the project direction and implementation details.\nAI as Collaborator, Not Replacement - AI should ask critical questions about your project requirements, architecture, and goals. The collaboration should be bidirectional, with you guiding the AI\u0026rsquo;s suggestions.\nPlan Before Implementation - Always create a comprehensive plan before diving into code. AI can help generate this plan, but you must review, validate, and refine it.\nThe Development Workflow Step 1: Create a Project Plan\nDefine clear project requirements and scope Ask AI to generate a plan based on your specifications Review the plan critically and request modifications Ensure the plan is detailed and unambiguous Step 2: Break Down into User Stories\nConvert the plan into user stories with clear acceptance criteria Divide large scope into smaller, manageable units Each unit becomes a mini-project that can be assigned to team members Estimate timelines for each unit (though be cautious of over-estimation) Step 3: Define Technology Stack\nClearly specify the technologies, frameworks, and tools to be used Instead of telling AI \u0026ldquo;don\u0026rsquo;t implement this,\u0026rdquo; tell it \u0026ldquo;implement this way\u0026rdquo; Positive direction yields higher success rates than negative constraints Step 4: Detailed Requirements \u0026amp; Design\nWrite requirements with precision and clarity Collaborate with AI to create detailed specifications Define data models, API contracts, and system architecture Create design documents before implementation begins Step 5: Implementation \u0026amp; Verification\nImplement features according to the plan Use mob development approach (team works together on code) Verify all output code as a team Conduct code reviews and quality checks Step 6: Testing \u0026amp; Deployment\nMove through environments: Development (Dev) → Testing (QA) → User Acceptance Testing (UAT) → Production (Prod) Ensure quality gates at each stage Validate functionality before production release Critical Success Factors Create a Plan First - Don\u0026rsquo;t expect AI to handle everything. Always start with a clear plan. Review Regularly - Continuously review AI suggestions and outputs. High error rates are possible. You Are the Manager - Your value lies in code validation and project management, not in writing every line of code. Ask Clarifying Questions - Ensure AI understands your project context by asking it critical questions about requirements, architecture, and goals. Use Prompt Templates - Create structured prompts that include user context, user stories, and specific requirements to get clearer AI responses. Export Plans to Files - Have AI generate plans as files you can save, review, and modify. This creates a living document for future reference. Be Polite to AI - Maintain respectful communication with AI tools. Good rapport may help in future interactions (and it\u0026rsquo;s just good practice!). 2. Amazon Q Developer Demonstration What is Amazon Q Developer? Amazon Q Developer is an AI-powered assistant that transforms the software development lifecycle (SDLC) through agentic capabilities across multiple platforms:\nAWS Console - Helps with infrastructure and service configuration IDE (Integrated Development Environment) - Provides code generation and optimization suggestions CLI (Command Line Interface) - Assists with command generation and automation DevSecOps Platforms - Integrates security practices into the development workflow Key Capabilities Code Generation \u0026amp; Quality\nAccelerates code generation with AI-powered suggestions Improves code quality through intelligent recommendations Maintains seamless integration with existing workflows Understands complex codebases and suggests optimizations Documentation \u0026amp; Testing\nAutomatically generates comprehensive documentation Creates unit tests with minimal manual effort Significantly improves code maintainability and reliability Reduces boilerplate and repetitive coding tasks Intelligent Collaboration\nActs as an intelligent collaborator leveraging large language models Combines deep AWS service knowledge with coding expertise Helps developers accelerate development cycles Enhances code quality and strengthens security posture Automation Across Development Lifecycle\nAutomates routine tasks across the entire development lifecycle Reduces manual, repetitive work Allows developers to focus on higher-value, creative tasks Improves overall productivity and efficiency Best Practices for Using Amazon Q Developer Provide Clear Context - Give Q detailed information about your project, architecture, and requirements Use Specific Prompts - Instead of vague requests, provide specific, detailed prompts with examples Review Suggestions - Always review Q\u0026rsquo;s suggestions before implementing them Iterate and Refine - If the first suggestion isn\u0026rsquo;t perfect, refine your prompt and try again Leverage AWS Knowledge - Take advantage of Q\u0026rsquo;s deep understanding of AWS services and best practices 3. Kiro Demonstration What is Kiro? Kiro is an agentic IDE (Integrated Development Environment) developed by Amazon Web Services that bridges the gap between rapid AI-powered prototyping and production-ready software development. It\u0026rsquo;s currently in public preview.\nCore Philosophy Kiro embodies the principle that AI should enhance developer productivity while maintaining professional standards, clear structure, comprehensive testing, documentation, and long-term maintainability.\nKey Features Spec-Driven Development\nWhen you submit a requirement (e.g., \u0026ldquo;add a product rating system\u0026rdquo;), Kiro converts it into: User stories with clear acceptance criteria Design documentation Task lists and implementation plans Structured specifications before code generation Agent Hooks \u0026amp; Automation\nAutomatically triggers tasks based on events: File saves trigger documentation updates Commits trigger test generation Specific actions trigger performance optimization Reduces manual, repetitive work Steering \u0026amp; Project Context\nCreate steering files (markdown) to describe: Project structure and organization Coding standards and conventions Desired architecture patterns Team guidelines and best practices Helps Kiro understand your project context deeply Multi-File Analysis \u0026amp; Intent Understanding\nAnalyzes multiple files simultaneously Understands functional goals across the codebase Makes changes aligned with overall project objectives Goes beyond simple code completion VS Code Integration\nBuilt on VS Code\u0026rsquo;s open-source foundation Import settings, themes, and extensions from VS Code Familiar interface for existing VS Code users Seamless transition for developers Flexible AI Model Selection\nCurrently uses Claude Sonnet 4 as default \u0026ldquo;Auto\u0026rdquo; mode combines multiple models based on context Balance between quality and cost Flexibility to choose different models for different tasks Advantages of Using Kiro Increased Transparency \u0026amp; Control\nStart with specifications before code generation Review and validate specs before implementation Reduce hallucinated code or misaligned implementations Maintain clear traceability from requirements to code Reduced Boilerplate \u0026amp; Repetitive Tasks\nAgent hooks automate documentation generation Automatic unit test creation Automatic information updates Frees developers for higher-value work Security \u0026amp; Privacy\nMost code operations happen locally Data only sent externally with explicit permission Maintains control over sensitive information Extensibility \u0026amp; Flexibility\nIntegrates external tools via MCP (Model Context Protocol) Supports multiple AI models Not locked into a single AI environment Adaptable to different team workflows Limitations \u0026amp; Considerations Preview Status - Still in public preview; stability and features may change Complex Projects - May struggle with deep contextual understanding in highly complex projects Supervision Required - Users still need to oversee and validate AI decisions Future Pricing - Expected pricing tiers: Free: ~50 tasks/month Pro: ~1,000 tasks/month Pro+: ~3,000 tasks/month When to Use Kiro You want an AI + programming workflow that maintains professionalism and clear structure Building rapid prototypes but concerned about production sustainability Exploring how AI can become a true programming colleague, not just a code suggestion tool You need spec-driven development with automated documentation and testing Common Pitfalls When Using AI in Development 1. Expecting AI to Handle Everything Problem: Many developers expect AI to complete entire projects autonomously.\nSolution: Always create a plan first and review regularly. AI is a tool to enhance productivity, not replace developer judgment.\n2. High Error Rates Problem: AI can make mistakes, especially in complex scenarios.\nSolution: Implement regular review cycles. Validate all AI-generated code before deployment.\n3. Lack of Clear Requirements Problem: Vague or unclear requirements lead to vague AI outputs.\nSolution: Write requirements with precision. Collaborate with AI to create detailed specifications before implementation.\n4. Negative Constraints Instead of Positive Direction Problem: Telling AI \u0026ldquo;don\u0026rsquo;t do this\u0026rdquo; is less effective than \u0026ldquo;do this.\u0026rdquo;\nSolution: Use positive, specific instructions. Higher success rates come from clear positive direction.\n5. Insufficient Project Context Problem: AI doesn\u0026rsquo;t understand your project\u0026rsquo;s unique requirements and constraints.\nSolution: Create steering files, provide detailed context, and ask AI critical questions about your project.\n6. Treating AI as a Manager Problem: Letting AI make all decisions about project direction and architecture.\nSolution: Remember: You are the manager. Your value lies in code validation and project oversight, not in writing every line of code.\nKey Takeaways AI is Your Assistant - Maintain control over project decisions and implementation direction\nPlan First, Code Second - Always create a comprehensive plan before implementation\nCollaboration Over Automation - AI should ask questions and collaborate, not just execute commands\nClear Requirements Matter - Precision in requirements leads to better AI outputs\nRegular Review is Essential - Don\u0026rsquo;t expect AI to be perfect; review and validate continuously\nYou Are the Code Manager - Your value is in validation and oversight, not in writing every line\nUse Structured Prompts - Templates with context, user stories, and requirements yield better results\nExport Plans to Files - Create living documents you can reference and modify\nPositive Direction Works Better - Tell AI what to do, not what to avoid\nExperience Matters - Use these tools hands-on to understand their capabilities and limitations\nRecommended Tools \u0026amp; Resources Amazon Q Developer - AI-powered development assistant integrated with AWS services Kiro IDE - Spec-driven development environment with AI collaboration AWS CodeWhisperer - Code generation and optimization tool MCP (Model Context Protocol) - Framework for integrating external tools and services Conclusion The AI-Driven Development Lifecycle represents a new paradigm in software engineering where AI and humans collaborate as equals. Success requires clear planning, regular review, precise requirements, and maintaining developer control over project direction. Tools like Amazon Q Developer and Kiro are enabling this new workflow, but they work best when developers understand their capabilities and limitations, and maintain their role as project managers and code validators.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/3-blogstranslated/3.2-blog2/",
	"title": "From Theory to Practice Using Amazon Q Developer CLI to Create Customized AWS Projects",
	"tags": [],
	"description": "",
	"content": "From Theory to Practice Using Amazon Q Developer CLI to Create Customized AWS Projects\nBy Ifeanyi Otuonye and Favour Ezeugwa | on August 26, 2025 | in Amazon Q Developer, AWS CLI, AWS Training and Certification | Permalink | Share.\nDo you have in-depth theoretical knowledge of AWS, perhaps through preparing for AWS Certification exams, but find it difficult to translate that knowledge into practical projects during your studies? You are not alone. Many cloud practitioners face a common challenge of how to move from theoretical understanding to practical implementation ideas. What if you had an AI-powered advisor that could not only suggest projects that match your skills and interests, but also provide step-by-step implementation guidance, helping to bridge the gap between existing knowledge and practical experience?\nIn this article, we will guide you through creating and deploying real-world projects on Amazon Web Services (AWS) using Amazon Q Developer command line interface (CLI). By combining the power of AI and CLI, you can build a personalized project generator that understands your skill level and helps you develop practical AWS solutions to add to your personal portfolio.\nWe designed this guide for people who are already AWS certified or who have a basic understanding of AWS and want to gain hands-on experience by building their own project portfolio. This includes people who are already AWS certified or who want to strengthen their practical skills. Through strategic prompts and deployment guidance, you will discover how Amazon Q Developer CLI can serve as both an AWS project advisor and a deployment assistant.\nUnderstanding Amazon Q Developer CLI\u0026rsquo;s Project Creation Capabilities\nUnlike searching through traditional documentation or template repositories, Amazon Q Developer CLI understands context and can generate customized project suggestions based on your skill level and goals. This capability goes beyond concise command suggestions and helps you build a practical learning path that grows with your expertise.\nWhen you interact with Amazon Q Developer CLI, you are using a tool that understands AWS best practices, security considerations, and common deployment patterns. Its real power lies in its ability to provide contextual guidance, explaining not only what to do but also why certain approaches are recommended.\nPrerequisites\nBefore you begin, you need the following prerequisites:\nAn AWS account with appropriate permissions in AWS Identity and Access Management (IAM), including AmazonS3FullAccess, IAM role permissions, and Amazon Q Developer CLI permissions (bedrock:InvokeModel and bedrock:InvokeModelWithResponseStream).\nYour local environment should have both AWS Command Line Interface (AWS CLI) and Amazon Q Developer CLI installed and configured to authenticate with your AWS account. For installation information, refer to Installing or updating to the latest version of the AWS CLI and Installing Amazon Q for command line. To get familiar with these tools, follow the AWS CLI Getting Started and Amazon Q Developer CLI tutorials in AWS Skill Builder. Using Amazon Q Developer follows a pay-as-you-go model, so monitor your usage through the AWS Management Console to effectively manage costs. Also, refer to the Amazon Q Developer pricing for specific pricing details.\nOnce setup is complete, you will see the Amazon Q Developer CLI chat interface.\n![][image1]\nCreating Effective Project Prompts\nThe quality of your project suggestions depends significantly on how you communicate with the Amazon Q Developer CLI. By carefully crafting your prompts, you can tap into specific project ideas that align with your current capabilities and learning goals. For example, if you are an AWS Certified Cloud Practitioner, Amazon Q Developer CLI can suggest projects that build on the concepts of this certification while also providing hands-on implementation experience.\nHere is a practical prompt example for the Amazon Q Developer CLI chat interface:\nYou are my AWS project advisor with expertise in helping beginners\nimplement practical AWS solutions. I am an AWS Certified Cloud Practitioner looking to build my first hands-on projects. Please suggest three beginner-friendly projects using Amazon S3 that:\n1. Build upon Cloud Practitioner knowledge\n2. Follow AWS best practices and the Well-Architected Framework\n3. Are free-tier friendly\n4. Have clear learning outcomes\n5. Can be completed within a few hours\nFor each project suggestion, please include:\n- The main AWS services involved\n- Key learning objectives\n- Estimated time to complete\n- Potential real-world applications\nWhen you send a prompt to the Amazon Q Developer CLI, the response will provide carefully structured suggestions that align with both your certification knowledge and your practical learning needs. From these suggestions, we will focus on the suggested project of deploying a static website hosting, an ideal starting point that combines basic AWS concepts with real-world results.\nAfter submitting the example prompt to the Amazon Q Developer CLI, it provides carefully structured suggestions that are relevant to both your certification knowledge and your practical learning needs. From these suggestions, we will focus on the project idea of ​​deploying a static website with Amazon CloudFront distribution. This beginner-friendly project combines three key AWS services: Amazon Simple Storage Service (Amazon S3) for hosting, Amazon CloudFront for global content delivery, and the optional Amazon Route 53 for domain management. With an estimated completion time of 2–3 hours, this is an ideal starting point for combining basic AWS concepts with practical results.\n![][image2]From Project Idea to Implementation Steps\nMoving from idea to implementation requires clear infrastructure definitions. Amazon Q Developer CLI excels at generating precise deployment commands while remaining beginner-friendly. Ask Amazon Q Developer CLI to provide deployment steps using the following prompt:\nAs my AWS project advisor, help me implement a simple version of the static website hosting solution using Amazon S3 from project 1.\nFor each step:\n1. Provide the exact AWS CLI commands needed\n2. Explain the purpose and importance of each command\n3. Include necessary security considerations\n4. Highlight best practices\n5. Mention potential gotchas or common mistakes to avoid\nThe generated response, including CLI steps and commands, demonstrates how Amazon Q Developer CLI can be a learning tool, illustrating how to properly configure an S3 bucket, set up website hosting, and the necessary permissions. It guides you through each command, explaining how the components work together to create a secure and efficient static website hosting solution.![][image3]\nDeploy and practice live\nWhile the Amazon Q Developer CLI can automate the execution of commands and manage the deployment process, the real learning comes from performing the steps yourself. Let\u0026rsquo;s deploy the first step from the generated response by creating an S3 bucket using the AWS CLI, as shown in the previous image.\nTo create your S3 bucket, open a new terminal window. Enter the following command from the Amazon Q Developer CLI-generated step to create an S3 bucket, replacing my-name with your own to ensure uniqueness:\naws s3 mb s3://my-portfilio-site-2025-my-name --region us-east-1\nAfter creating the S3 bucket, continue through each remaining deployment step, from enabling static website hosting to the final step of getting the static website endpoint URL. Make sure you understand the purpose of each command. If you get stuck, the Amazon Q Developer CLI will assist you; if you run into problems or need clarification, ask the Amazon Q Developer CLI for help. It will provide troubleshooting guidance, explaining potential causes and solutions. This process of encountering and resolving issues will be an important part of your learning journey. After completing all the remaining steps to deploy the static website and getting the endpoint URL, copy and paste it into your browser to view your live website. If everything goes well,Well done, congratulations on successfully deploying your first AWS project with Amazon Q Developer CLI!\nInfrastructure as Code with Amazon Q Developer CLI\nNow that you have manually deployed your static website, you can explore other powerful features of Amazon Q Developer CLI, such as creating Infrastructure as Code (IaC).\nAsk Amazon Q Developer CLI to generate an AWS Cloud Development Kit (AWS CDK) stack that represents your deployment and provides an explanation of its components:\nCreate a CDK TypeScript stack for the S3 static website we just built manually.\nAfter generating the code, please explain:\n- The main components of the CDK stack\n- How each component relates to our manual implementation steps\n- Any best practices or optimizations included in the generated code\n- How this CDK implementation enhances our project\u0026rsquo;s maintainability and scalability\nThe Amazon Q Developer CLI will generate a complete CDK implementation and provide detailed explanations, turning your manual steps into infrastructure as code with additional context. You can use this approach to manage infrastructure versions, ensure consistent deployments across environments, and solidify your understanding of AWS services and IaC principles.\nBuilding on your success\nAfter successfully deploying a static website to Amazon S3, you can consider improving its performance and security. A natural enhancement is to add Amazon CloudFront, a content delivery network (CDN) that delivers website content faster to users globally while enhancing security. To learn how to get started with CloudFront, visit Amazon CloudFront Getting Started in AWS Skill Builder.\nThe Amazon Q Developer CLI can guide you through this improvement. Use natural language to ask it to explain how to add CloudFront to your existing Amazon S3 static website, and it will provide the necessary steps and commands. This improvement will improve your website performance and introduce important concepts such as CDNs, edge locations, and cache behaviors.\nClean up your resources\nAfter completing your static website project, it is important to remove resources that are no longer needed. This practice helps you avoid unnecessary costs and reinforces the cost optimization principles of the Well-Architected Framework. To delete the resources created in this tutorial, you can ask Amazon Q Developer CLI to guide you through the cleanup process using the following prompt:\nProvide the commands in the correct order to help me clean up all the AWS resources we created for the static website project, including the S3 bucket, its contents, etc.\nAmazon Q Developer CLI will provide you with the appropriate AWS CLI commands to securely delete the resource. Typically, this involves deleting all objects in the S3 bucket first, then deleting the bucket itself. If you have upgraded your project with CloudFront, you will also need to disable and delete the distribution. This practice will be useful as you progress to more complex projects with multiple AWS services and potentially higher costs.\nIntegration with AWS Training Resources\nWhile Amazon Q Developer CLI provides robust project guidance, it works best when combined with the comprehensive learning resources that AWS provides. The hands-on experience from this project complements the training courses available on AWS Skill Builder. For example, the Introduction to Amazon S3 course provides deeper context on static website hosting capabilities, while the “AWS Command Line Interface Basics” course helps you understand CLI commands better.\nBuild a learning rhythm that combines project work with Amazon Q Developer CLI and structured AWS training courses. Start with the basics on AWS Skill Builder, implement them using the Amazon Q Developer CLI tutorial, and then reflect on what you’ve built. This approach solidifies your knowledge while creating real-world products for your portfolio.\nConclusion\nThe journey from AWS Certification knowledge to real-world implementation doesn’t have to be difficult. By using the Amazon Q Developer CLI as a project advisor and implementation guide, you’ve discovered a simple approach to building project-based AWS solutions. This approach will help you create portfolio-worthy projects and deepen your understanding of AWS services through hands-on experience. Remember, every AWS expert starts with simple projects like the static website you just deployed following the instructions in this article. As you gain confidence, use the Amazon Q Developer CLI to explore more complex architectures and implementations. The hands-on experience from these projects, combined with the training resourcesAWS, will provide a solid foundation for your learning journey.\nAdditional Resources\nTo continue your learning journey, explore the following AWS resources:\nAWS Command Line Interface Basics\nAmazon Q Developer CLI User Guide\nAmazon S3 Getting Started\nAmazon CloudFront Getting Started\nWe encourage you to start with the static website project presented in this article. Once you are comfortable, upgrade the project with CloudFront to learn about CDNs. Share your implementations, ask questions in the AWS community forums, and keep building. Your next AWS project is just a prompt away.\nAbout the Authors\nIfeanyi Otuonye Ifeanyi Otuonye is a Technical Account Manager and a 10-time AWS Certified professional. As a strategic advisor, he helps customers achieve their business goals with AWS Cloud services. Passionate about education and inspiration, he develops content and resources that make cloud learning accessible, empowering professionals to grow in knowledge, influence, and impact. You can connect with him on LinkedIn.\nFavour Ezeugwa Favour Ezeugwa is a Solutions Architect at Amazon Web Services (AWS), where she helps customers design scalable, secure, and cost-effective cloud architectures. With a background in computer information systems and a passion for generative AI and cloud security, Favour has led key projects including the development of AI assistants and serverless applications. She is an advocate for hands-on learning, mentorship, and using technology to create meaningful change.\nSource:https://aws.amazon.com/en/blogs/training-and-certification/from-theory-to-practice-using-amazon-q-developer-cli-to-generate-tailored-aws-project s/\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.3-lambda-basics/5.3.2-hello-python/",
	"title": "Lambda Hello World (Python)",
	"tags": [],
	"description": "",
	"content": "Create the function Lambda console -\u0026gt; Create function -\u0026gt; Author from scratch. Name: hello-python, Runtime: Python 3.12. Handler: import json def lambda_handler(event, context): name = ( (event.get(\u0026#34;queryStringParameters\u0026#34;) or {}).get(\u0026#34;name\u0026#34;) or event.get(\u0026#34;name\u0026#34;) or \u0026#34;world\u0026#34; ) return { \u0026#34;statusCode\u0026#34;: 200, \u0026#34;headers\u0026#34;: {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;}, \u0026#34;body\u0026#34;: json.dumps({\u0026#34;message\u0026#34;: f\u0026#34;Hello, {name}\u0026#34;}) } Save, create a test event (e.g., {\u0026quot;name\u0026quot;:\u0026quot;Bob\u0026quot;}), and view CloudWatch logs. Optional: add a 400 response for missing/invalid input. "
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.2-prerequisite/",
	"title": "Prerequisite",
	"tags": [],
	"description": "",
	"content": "Account \u0026amp; Permissions An AWS account with permissions to create Lambda functions, basic IAM roles, CloudWatch Logs, and API Gateway. No advanced EC2/VPC permissions are required for this lab. Tools A web browser to work with the AWS Console. curl or Postman to invoke the API Gateway endpoint. Optional: a local editor to draft Node.js/Python code before pasting into the console. Quick Setup Select a region close to you (e.g., us-east-1 or ap-southeast-1). Create an IAM role for Lambda with the AWSLambdaBasicExecutionRole permission. "
},
{
	"uri": "http://localhost:1313/hugo_aws/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Online Library – A Serverless Content Platform for Small Groups 1. Executive Summary The Online Library project aims to build a low-cost serverless platform for storing and distributing content (PDF/ePub) for a small user group (initially ~100 users, primarily students/labs needing controlled internal research-material sharing). The solution prioritizes security, content moderation (Admin Approval), and transparent, linear operating costs as it scales.\nThe architecture uses a fully AWS Serverless stack (Amplify, Cognito, API Gateway, Lambda, S3, CloudFront, DynamoDB).\nEstimated cost for the MVP (excluding Free Tier) is ≈ $9.80/month, with predictable scaling to 5,000–50,000 users.\n2. Problem Statement What’s the Problem? Documents and books are scattered; there is no secure content delivery system with access control; the process of adding or moderating user-generated content (UGC) is slow and has high friction.\nThe Solution: Build a serverless pipeline on AWS:\nUsers upload files via Presigned PUT URL to temporary S3; Admin approves → Lambda moves the file to a protected public folder; Readers access content via Signed GET URL (from CloudFront/CDN) to ensure speed and controlled access.\nBenefits and Return on Investment Business value: Centralized content; quality control through moderation; fast deployment with CI/CD. Technical benefits: Very low operating cost (≈ $9.80/month for MVP); scalable serverless architecture; secured content access. 3. Solution Architecture A. High level B. Request flow AWS Services Used Service Primary Role Specific Tasks Amplify Hosting CI/CD + FE Hosting Build \u0026amp; Deploy Next.js, domain management Cognito Authentication Sign-up/Login, JWT issuance, refresh tokens API Gateway API Entry Point Receive requests, validate JWT, route to Lambda Lambda Business Logic Handle upload/approval, generate signed URLs, write metadata S3 Object Storage Store original and approved files, served via CloudFront Signed URL CloudFront CDN Fast content delivery, blocks direct S3 access via OAC DynamoDB Database Store metadata (title, uploader, approval status) Route 53 DNS Domain mapping to Amplify, API Gateway, CloudFront CloudWatch Monitoring Lambda logs, anomaly alerts Search Simple search fields (titles, author) using DynamoDB GSIs.\nComponent Design User Upload: Presigned PUT to S3 uploads/. Admin Approval: Lambda copies file from uploads/ → public/books/ upon approval. Reader Security: CloudFront OAC prevents direct S3 access; reading occurs only through Signed URL generated by Lambda. Search Architecture Simple Search: Design GSI for title and author (example: GSI1: PK=TITLE#{normalizedTitle}, SK=BOOK#{bookId}; GSI2: PK=AUTHOR#{normalizedAuthor}, SK=BOOK#{bookId}). Add endpoint GET /search?title=...\u0026amp;author=... to query GSI instead of Scan. Admin Authorization Use Cognito User Groups with an Admins group. Admin JWT contains cognito:groups: [\u0026quot;Admins\u0026quot;]. Admin-specific Lambdas (exampleapproveBook, takedownBook) check this claim and return 403 Forbidden otherwise. JWT Authorizer (API Gateway HTTP API) handles authentication, while authorization logic is inside Lambda. 4. Technical Implementation Implementation Phases Design \u0026amp; IaC: Use CDK to define all stacks (Cognito, DDB, S3, Amplify, Lambda, API). Upload \u0026amp; Approval Flow: Implement Presigned PUT, metadata (status= PENDING), Admin approval logic. Reading Flow: Implement Signed GET, FE reader stream via CloudFront. Ops: CloudWatch logs, budget alerts, IAM hardening. Search: Add GSI for title, author, implement GET /search. Technical Requirements Entire infrastructure defined using CDK. API Gateway uses HTTP API for cost savings. Lambda (Python) handles business logic \u0026amp; DynamoDB/S3. S3 Bucket Policy must deny public access and allow only CloudFront OAC. 5. Timeline \u0026amp; Milestones Project Timeline Platform \u0026amp; Authentication (Week 1–2) Objective: Set up infrastructure and allow user login.\nBackend Tasks (CDK/DevOps): CDK/IaC stack for Cognito. CDK stack for DynamoDB (main table, no GSI yet). CDK stack for S3 (uploads, public, logs) + OAC. Deploy API Gateway (HTTP API) + a test Lambda. Frontend Tasks (Amplify): Configure Amplify Hosting + GitHub CI/CD. Integrate Amplify UI / Cognito SDK for: Sign-up, Email verification, Login, Forgot password. Milestone: git push automatically deploys FE. User can sign-up/login and obtain JWT. Upload \u0026amp; Approval Flow (Week 2–3) Objective: Allow authenticated users to upload files and Admins to approve them.\nBackend (Lambda/CDK): Implement createUploadUrl Lambda: Validate JWT. Create Presigned PUT URL to uploads/. Write metadata (status=PENDING). Implement approveBook: Validate Admin role. Copy S3 file uploads/ → public/books/. Update DynamoDB status (APPROVED). Frontend: Upload form (drag \u0026amp; drop). Upload via Presigned PUT. Admin dashboard with list of PENDING, button “Approve”. Reading \u0026amp; Search (Week 3–4) Objective: Allow reading \u0026amp; searching approved books.\nBackend: Implement getReadUrl: generate Signed GET URL (short TTL). Add GSI for title, author. Implement searchBooks. Frontend: Homepage: book list. Search bar → API searchBooks. Reader screen using the Signed URL (e.g., via react-pdf). Ops \u0026amp; Security (Week 5–6) Backend: S3 Event Notification for new uploads. Lambda validateMimeType: read magic bytes to verify PDF/ePub. Lambda takedownBook (Admin), deleteUpload (auto cleanup after 72h). DevOps: AWS Budget Alerts, CloudWatch Alarms. IAM least-privilege + CORS tightening. 6. Budget Estimation Budget comes from AWS Pricing Calculator.\nMonthly cost (strict, no Free Tier, ~100 users): ≈ $9.80/month.\n# AWS Service Region Monthly (USD) Notes 0 Amazon CloudFront Asia Pacific (Singapore) 0.86 10 GB data egress + 10 000 HTTPS requests 1 AWS Amplify Asia Pacific (Singapore) 1.31 100 build min + 0.5 GB storage + 2 GB served 2 Amazon API Gateway Asia Pacific (Singapore) 0.01 ~10 000 HTTP API calls/tháng 3 AWS Lambda Asia Pacific (Singapore) 0.00 128 MB RAM × 100 ms × 10 000 invokes 4 Amazon S3 (Standard) Asia Pacific (Singapore) 0.05 2 GB object storage for books/images 5 Data Transfer Asia Pacific (Singapore) 0.00 Included in CloudFront cost 6 DynamoDB (On-Demand) Asia Pacific (Singapore) 0.03 Light metadata table (0.1 GB, few reads/writes) 7 Amazon Cognito Asia Pacific (Singapore) 5.00 100 MAU, Advanced Security enabled 8 Amazon CloudWatch Asia Pacific (Singapore) 1.64 5 metrics + 0.1 GB logs/tháng 9 Amazon Route 53 Asia Pacific (Singapore) 0.90 1 Hosted Zone + DNS queries ≈ 9.80 USD / month No Free Tier applied Infrastructure Costs This cost model demonstrates the efficiency of serverless architecture: costs are primarily centered on the value delivered to the user (Cognito MAU), rather than paying for \u0026lsquo;idle servers\u0026rsquo;.\n7. Risk Assessment Risk Matrix Risk Impact Mitigation Cost spike due to sudden user growth High Limit MAU, cache metadata via CloudFront Abuse of uploads Medium Limit ≤ 50MB; auto-delete after 72h Fake/malicious file types Medium S3 Event → Lambda MIME validation Monitoring overload Low CloudWatch alerts, 14-day retention Mitigation Strategies cost: Set AWS Budget Alerts for CloudFront and Cognito. Be aware that Signed URLs have a short TTL and should not be cached publicly long-term; instead, cache metadata/API responses (book lists, details) on CloudFront for 3–5 minutes to reduce API load. Only generate Signed URLs when the user actually clicks to read (on-demand), do not pre-generate for the entire list. Upload: Limit file size to ≤ 50MB for MVP. (Can be increased to 200MB if needed, use multipart upload on the FE to avoid timeouts.) Apply Rate Limit/Throttling on API Gateway for endpoints that create Presigned URLs. Set up an S3 Lifecycle Policy to automatically delete unapproved files in uploads/ after 72h. Add Server-side Validation: S3 Event Notifications $\\to$ Lambda reads magic bytes (e.g., file-type library) to verify correct PDF/ePub; if incorrect, automatically delete and write REJECTED_INVALID_TYPE status to DynamoDB. Copyright (DMCA): Store Audit Log in DynamoDB: uploaderID, uploadTimestamp, adminApproverID, approvalTimestamp for traceability. Build a Takedown API (Admin only): update status to TAKEDOWN; optionally move the object from public/books/ to quarantine/books/ (do not delete completely) to preserve traces. Contingency Plans If costs exceed budget, enable Invite-Only mode to cap Cognito MAU and reduce load.\n8. Expected Outcomes Technical Improvements Fast and secure content delivery (CDN + Signed URL). Standard AWS Serverless architecture capable of scaling to 50,000 users without redesign. Fully automated CI/CD for both frontend \u0026amp; backend. Long-term Value A centralized content platform for structured book data. Continuous documentation of an end-to-end Serverless implementation. Room for future analytics (QuickSight) or AI/ML features. This system proves the ability to build a platform that securely, cost-effectively, and scalably easy by AWS Serverless services - that suitable to apply for small groups or communities.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.2-week2/",
	"title": "Week 2 - AWS Networking Services",
	"tags": [],
	"description": "",
	"content": "Week: 2025-09-15 to 2025-09-19\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 2 Overview This week focused on AWS networking services, spanning foundational VPC concepts through enterprise-grade connectivity patterns.\nKey Topics Amazon VPC and subnet design Security Groups and NACLs Internet Gateway and NAT Gateway VPC Peering and Transit Gateway Elastic Load Balancing (ALB, NLB, GWLB) Hands-on Labs Lab 03: Amazon VPC \u0026amp; Networking Basics Lab 10: Hybrid DNS (Route 53 Resolver) Lab 19: VPC Peering Lab 20: AWS Transit Gateway Day 6-Amazon VPC (Virtual Private Cloud): Virtualizes an isolated logical data center on the AWS platform. You have complete control over this virtual network environment, including IP address ranges, Subnet configuration, routing tables, and gateways.\nSubnet: A partition of the VPC. A subnet is associated with a specific Availability Zone (AZ).\nPublic Subnet: Contains resources that need to access the Internet (e.g., web servers), using an Internet Gateway to connect.\nPrivate Subnet: Contains internal resources (e.g., databases), not directly accessing the Internet.\nDay 7\nSecurity Group (SG): Acts as an Instance/Network Interface level traffic filter.\nNetwork ACL (NACL): Acts as a Subnet level firewall that protects all resources within that Subnet.\nDay 8\nInternet Gateway (IGW): Allows resources in the Public Subnet to connect to the Internet (Provides outbound and inbound connections). Attached to a VPC\nNAT Gateway (Network Address Translation) Allows resources in the Private Subnet to connect to the Internet (e.g., to download patches), but prevents inbound connections from the Internet. Placed in a Public Subnet\nDay 9\nVPC Peering: Sets up a network connection between two VPCs so they can communicate with each other using private IP addresses. Note: Only works in a one-to-one (1-1) and non-transitive manner – if VPC A is peering with B, and B is peering with C, A and C cannot communicate directly.\nAWS Transit Gateway: A central network router. Allows multiple VPCs and on-premises networks to be easily connected and transitive. Ideal for large and complex networks.\nDay 10-Elastic Load Balancing (ELB): A service that distributes traffic to multiple targets (such as EC2 Instances) to increase application availability, scalability, and fault tolerance.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/3-blogstranslated/3.3-blog3/",
	"title": "Deploying Advanced AWS Graviton Adoption Strategies Across AWS Regions",
	"tags": [],
	"description": "",
	"content": "Deploying Advanced AWS Graviton Adoption Strategies Across AWS Regions\nBy Matt Howard | on August 26, 2025 | in Advanced (300), Amazon EC2, Best Practices, Graviton | Permalink | Share\nImplementing Advanced AWS Graviton Adoption Strategies Across AWS Regions by Matt Howard on 26 AUG 2025 in Advanced (300), Amazon EC2, Best Practices, Graviton Permalink Share\nAWS Graviton Processors can deliver cost savings, improved performance, and reduced carbon footprint when using Amazon Elastic Compute Cloud (Amazon EC2) instances. When scaling Graviton deployments across multiple AWS Regions, careful planning helps you consider regional instance type availability factors and optimize capacity. This article explains how to implement advanced configuration strategies for Graviton-enabled EC2 Auto Scaling groups across multiple Regions, helping you maximize instance availability, reduce costs, and maintain consistent application performance even in Regions with Graviton instance type constraints.\nFlexible Instance Type Strategies\nOne of the most effective strategies for maximizing Graviton availability is to flexibly use a variety of instance types and families. Instance families (such as m7g, c7g, and r7g) group similar instances of different sizes, each offering increasing proportions of vCPU and memory. When configuring EC2 Auto Scaling groups, try to use at least 10 instance types rather than limiting yourself to just one or two specific types. EC2 Auto Scaling supports this flexibility through a mixed instance group, which allows you to specify multiple instance types in the same group. See the example AWS CloudFormation snippet for the EC2 Auto Scaling group [MixedInstancesPolicy](https://docs.aws.amazon.com/AWSCloudFormation/latest/TemplateReference/aws-resource-autoscaling-autoscalinggroup.html cfn-autoscaling-autoscalinggroup-mixedinstancespolicy), which specifies two Graviton instance types belonging to two different families:\n\u0026ldquo;MixedInstancesPolicy\u0026rdquo;: {\n\u0026ldquo;Overrides\u0026rdquo;: [\n{\n\u0026ldquo;InstanceType\u0026rdquo;: \u0026ldquo;m7g.large\u0026rdquo;\n},\n{\n\u0026ldquo;InstanceType\u0026rdquo;: \u0026ldquo;c7g.xlarge\u0026rdquo;\n}\n]\n}\nThis limited choice significantly reduces access to available capacity pools. Assuming this workload requires at least 2 vCPUs and 8 GiB of memory, you can add the following eight additional Graviton instance types: m6g.large, m8g.large, m6gd.large, m7gd.large, m8gd.large, c6g.xlarge, c6gd.xlarge, and c8g.xlarge. This helps you meet the recommendation of scaling to 10 instance types. While some of these instance types have different prices, you can manage these cost impacts through allocation strategies discussed later in the article.\nTo efficiently identify all compatible Graviton instance types for your workload, you can use GetInstanceTypesFromInstanceRequirements. This approach eliminates the manual effort of researching and selecting each individual instance type.\naws ec2 get-instance-types-from-instance-requirements \\ --architecture-types arm64 \\ --virtualization-types hvm \\ --instance-requirements \u0026lsquo;{\u0026ldquo;VCpuCount\u0026rdquo;: {\u0026ldquo;Min\u0026rdquo;: 2,\u0026ldquo;Max\u0026rdquo;:8}, \u0026ldquo;MemoryMiB\u0026rdquo;: {\u0026ldquo;Min\u0026rdquo;: 8000}, \u0026ldquo;InstanceGenerations\u0026rdquo;:[\u0026ldquo;current\u0026rdquo;]}\u0026rsquo; \\ --region us-east-1\nThis example command returns dozens of compatible Graviton instance types from various families (c7g, c7gd, c7gn, m7g, m7gd, etc.), thereby expanding your capacity options. An EC2 Auto Scaling group\u0026rsquo;s mixed instance policy can allow [up to 40 instance types](https://docs.aws.amazon.com/autoscaling/ec2/APIReference/API_LaunchTemplateOverrides.html API_LaunchTemplateOverrides_Contents), giving you even more flexibility.\nAfter expanding your instance type choices, you need to configure how EC2 Auto Scaling chooses between available instance types. The [OnDemandAllocationStrategy](https://docs.aws.amazon.com/AWSCloudFormation/latest/TemplateReference/aws-properties-autoscaling-autoscalinggroup-instancesdistribution.html cfn-autoscaling-autoscalingggroup-instancesdistribution-ondemandallocationstrategy) in CloudFormation controls this behavior, providing two approaches: “lowest-price” and “prioritized”. With the “lowest-price” strategy, EC2 Auto Scaling will launch instances from the lowest-priced capacity pool available:\n\u0026ldquo;OnDemandAllocationStrategy\u0026rdquo;: \u0026ldquo;lowest-price\u0026rdquo;\nThis strategy helps manage costs when you have included multiple instance types. Even with increased instance type flexibility, your workloads will automatically choose the most cost-effective option from the available capacity pools. Alternatively, you can use the “prioritized” strategy if you want more control over which instance types are selected first:\n\u0026ldquo;OnDemandAllocationStrategy\u0026rdquo;: \u0026ldquo;prioritized\u0026rdquo;\nRegional adaptation techniques\nNot all AWS Regions have the same Graviton instance types available. Differences in instance type availability between regions create challenges when deploying applications consistently across multiple AWS Regions. To address these differences, extend instance type flexibility beyond the minimum of 10 types, ensuring there are enough options in each AWS Region you operate.\nTo implement this flexibility across AWS Regions, you need to determine which Graviton instance types are available in each target region. AWS provides multiple methods to access this information: check the Amazon EC2 Instance Types by Region documentation for a complete list, use the DescribeInstanceTypeOfferings Amazon EC2 API to determine the available types programmatically, or visit the [EC2 Instance Types page in the AWS Management Console](https://console.aws.amazon.com/ec2/home InstanceTypes:v=3).\nYou can also run the GetInstanceTypesFromInstanceRequirements API on different AWS Regions to understand regional differences. For example, running the same queries in US East (N. Virginia) and Asia Pacific (Taipei) shows a significant difference: over 70 compatible instance types in US East (N. Virginia) and 27 in Asia Pacific (Taipei).\n\\ Query for US East (N. Virginia) aws ec2 get-instance-types-from-instance-requirements \\ --architecture-types arm64 \\ --virtualization-types hvm \\ --instance-requirements \u0026lsquo;{\u0026ldquo;VCpuCount\u0026rdquo;: {\u0026ldquo;Min\u0026rdquo;: 2,\u0026ldquo;Max\u0026rdquo;:8}, \u0026ldquo;MemoryMiB\u0026rdquo;: {\u0026ldquo;Min\u0026rdquo;: 8000}, \u0026ldquo;InstanceGenerations\u0026rdquo;:[\u0026ldquo;current\u0026rdquo;]}\u0026rsquo; \\ --region us-east-1\n\\ Query for Asia Pacific (Taipei) aws ec2 get-instance-types-from-instance-requirements \\ --architecture-types arm64 \\ --virtualization-types hvm \\ --instance-requirements \u0026lsquo;{\u0026ldquo;VCpuCount\u0026rdquo;: {\u0026ldquo;Min\u0026rdquo;: 2,\u0026ldquo;Max\u0026rdquo;:8}, \u0026ldquo;MemoryMiB\u0026rdquo;: {\u0026ldquo;Min\u0026rdquo;: 8000}, \u0026ldquo;InstanceGenerations\u0026rdquo;:[\u0026ldquo;current\u0026rdquo;]}\u0026rsquo; \\ --region ap-east-2\nWhen operating across multiple AWS Regions, design a single mixed instance policy that works across all regions by including instance types that are available across all AWS Regions you operate in. Based on the previous query, you can include the following 10 instance types that are available across both AWS Regions: m6g.large, m7g.large, m6gd.large, m7gd.large, c6g.xlarge, c7g.xlarge, m6g.xlarge, m7g.xlarge, c6gn.xlarge, and m6gd.xlarge.\nYou should also deploy your EC2 Auto Scaling group across multiple Availability Zones (AZs) for increased fault tolerance and access to deeper capacity pools. To determine which AZs are available in your AWS Region, refer to the Availability Zones documentation or check the Amazon Virtual Private Cloud (Amazon VPC) to determine which AZs are being used by subnets using the Amazon EC2 API. Configure the EC2 Auto Scaling group to use all available AZs using the [AvailabilityZones](https://docs.aws.amazon.com/AWSCloudFormation/latest/TemplateReference/aws-resource-autoscaling-autoscalinggroup.html cfn-autoscaling-autoscalinggroup-availabilityzones) parameter in CloudFormation AWS::AutoScaling::AutoScalingGroup:\n\u0026ldquo;AvailabilityZones\u0026rdquo;: [ \u0026ldquo;us-west-2a\u0026rdquo;, \u0026ldquo;us-west-2b\u0026rdquo;, \u0026ldquo;us-west-2c\u0026rdquo;, \u0026ldquo;us-west-2d\u0026rdquo;, ]\nBest Practices for Using EC2 Spot Instances with Graviton-Based Instances\nWhile optimizing regional availability and AZ allocation provides a solid foundation, enhancing your Graviton deployment strategy with configuration Amazon EC2 Spot Instances can significantly improve cost efficiency without sacrificing reliability. When using Spot Instances with Graviton, you should implement strategies that maximize your chances of acquiring and maintaining capacity.\nFirst, Spot Instance Advisor provides useful information about the frequency of interruptions of different instance types on AWS Regions. Use this tool to identify Graviton instance types with lower interruption rates in your target regions. Then, expand your mixed instance group to include these instance types. Especially for workloads using Spot Instances, maximize instance type flexibility by specifying up to a limit of 40 instance types for EC2 Auto Scaling groups mixed instance policies. This broad selection increases your chances of finding available Spot Instances capacity.\nIn addition to instance type selection, the allocation strategy you choose significantly affects your ability to maintain Spot Instances capacity. Configure the Spot allocation strategy using the AWS::AutoScaling::AutoScalingGroup InstancesDistribution property with parameters [SpotAllocationStrategy](https://docs.aws.amazon.com/AWSCloudFormation/latest/TemplateReference/aws-properties-autoscaling-autoscalinggroup-instancesdistribution.html cfn-autoscaling-autoscalinggroup-instancesdistribution-spotallocationstrategy) set is[price-capacity-optimized](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-fleet-allocation-strategy.html ec2-fleet-allocation-strategies-for-spot-instances), to select Spot pools with the lowest risk of disruption while still considering price:\n\u0026ldquo;InstancesDistribution\u0026rdquo;: { \u0026ldquo;SpotAllocationStrategy\u0026rdquo;: \u0026ldquo;price-capacity-optimized\u0026rdquo; }\nFor workloads that can benefit from more time than the standard two-minute Spot disruption notification, enable Capacity Rebalancing. This feature, configured via the [AWS::AutoScaling::AutoScalingGroup CapacityRebalance](https://docs.aws.amazon.com/AWSCloudFormation/latest/TemplateReference/aws-resource-autoscaling-autoscalinggroup.html cfn-autoscaling-autoscalinggroup-capacityrebalance) property, enables EC2 Auto Scaling to proactively respond to rebalancing recommendations by launching a new Spot Instance two minutes before the running instance receives a Spot interruption notification, thus providing additional time to smoothly transition:\n\u0026ldquo;CapacityRebalance\u0026rdquo;: true\nFor maximum flexibility and better capacity access, consider combining both x86 and ARM architectures in launch templates. While Graviton capacity pools are newer and sometimes smaller than x86, the hybrid architecture approach ensures you can still launch instances even if one architecture is limited. For detailed instructions, see the AWS article: Supporting AWS Graviton2 and x86 instance types in the same Auto Scaling group.\nAttribute-based instance type selection\nWhile mixed instance policies with a list of specific instance types provide great flexibility, AWS also provides a more powerful method for dynamically selecting instance types: attribute-based instance type selection. This method makes management easier by allowing you to specify the attributes your application needs instead of specific instance types, automatically adapting to new instance types, and handling regional availability differences.\nYou can implement property-based instance type selection in the EC2 Launch Template via the [AWS::EC2::LaunchTemplate InstanceRequirements] attribute(https://docs.aws.amazon.com/AWSCloudFormation/latest/TemplateReference/aws-properties-ec2-launchtemplate-instancerequirements.html):\n{\n\u0026ldquo;InstanceRequirements\u0026rdquo;: {\n\u0026ldquo;AcceleratorCount\u0026rdquo;: {\n\u0026ldquo;Max\u0026rdquo;: 0\n},\n\u0026ldquo;BareMetal\u0026rdquo;: \u0026ldquo;excluded\u0026rdquo;,\n\u0026ldquo;BaselinePerformanceFactors\u0026rdquo;: {\n\u0026ldquo;Cpu\u0026rdquo;: {\n\u0026ldquo;References\u0026rdquo;: [\n{\n\u0026ldquo;InstanceFamily\u0026rdquo;: \u0026ldquo;c7g\u0026rdquo;\n}\n]\n}\n},\n\u0026ldquo;InstanceGenerations\u0026rdquo;: [\n\u0026ldquo;current\u0026rdquo;\n],\n\u0026ldquo;MemoryMiB\u0026rdquo;: {\n\u0026ldquo;Min\u0026rdquo;: 8000\n},\n\u0026ldquo;VCpuCount\u0026rdquo;: {\n\u0026ldquo;Min\u0026rdquo;: 4\n}\n}\n}\nBaselinePerformanceFactors in AWS::EC2::LaunchTemplate InstanceRequirements ensures [performance protection](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-fleet-attribute-based-instance-type-selection.html ec2fleet-abis-performance-protection). This feature ensures that EC2 Auto Scaling groups use instance types that meet or exceed the defined baseline performance. When you specify an instance family like “c7g” as the baseline reference, Amazon EC2 automatically excludes instance types below this performance level, even if they match the other attributes specified. With Graviton deployments, specifying “c7g” ensures onlyselect instance types that have equivalent or better performance than Graviton3 processors.\nProperty-based instance type selection also allows specifying instance types in the template that may not be available in an AWS Region by using [AllowedInstanceTypes](https://docs.aws.amazon.com/AWSCloudFormation/latest/TemplateReference/aws-properties-ec2-launchtemplate-instancerequirements.html cfn-ec2-launchtemplate-instancerequirements-allowedinstancetypes):\n{\n\u0026ldquo;AllowedInstanceTypes\u0026rdquo;: [\n\u0026ldquo;m6g.large\u0026rdquo;,\n\u0026ldquo;m7g.large\u0026rdquo;,\n\u0026ldquo;m8g.large\u0026rdquo;\n]\n}\nThis approach helps EC2 Auto Scaling groups use new instance types when available and deploy them automatically in other AWS Regions as they become available. This single template simplifies the deployment and management of instance selection in EC2 Auto Scaling groups across multiple regions.\nSpecial Notes\nThe following special considerations should be noted.\nPerformance Testing with Multiple Instance Types\nWhen deploying instance type flexibility, a common concern is that all instance types need to be tested with the application. Testing 40 different instance types is impractical for most organizations. Instead, consider the following approaches to reduce the amount of testing while still ensuring performance:\nGraviton instance families within the same generation (e.g., c7g, m7g, r7g) use the same processor, providing similar performance. Therefore, you can include multiple instance types from the same generation when testing a representative instance.\nConsider including variants within the family (e.g., c7gd with NVMe storage), as they provide specialized capabilities without changing the underlying CPU architecture.\nFor maximum flexibility, include multiple instance generations.\nIf an application runs well on Graviton3, it will most likely run better on Graviton4, allowing you to specify both in an EC2 Auto Scaling group.\nReserving a Specific Graviton Type\nIf a workload requires a specific Graviton instance type, use EC2 Capacity Reservations, which allows you to reserve capacity for EC2 instances in a specific AZ for any period of time.\nOn-Demand Capacity Reservations (ODCR) are for immediate use and do not require a term commitment.\nFuture-dated Capacity Reservations allow you to specify when capacity needs to be available along with a commitment period.\nAmazon EMR workloads\nAlthough Amazon EMR clusters exist only in a single AZ, you can use Amazon EMR instance fleets to select multiple subnets across different AZs. When you initialize a cluster, Amazon EMR searches across these subnets for specific instances and purchasing methods, thereby accessing a deeper capacity pool. With Instance Fleets, you can assign up to 30 EC2 instance types to each primary, core, and task node group, significantly improving instance flexibility and availability. See also: Responding to Amazon EMR cluster insufficient instance capacity events.\nConclusion\nThis article presents advanced strategies for maximizing AWS Graviton adoption across multiple AWS Regions. You can use the provided AWS CloudFormation examples as templates for your own deployments. Following these practices helps maintain consistent application performance and maximize Graviton instance availability across all AWS Regions you operate, even as Graviton availability continues to scale across AWS\u0026rsquo;s global infrastructure. For a comprehensive guide to maximizing Graviton deployments, see the AWS Graviton Technical Guide.\nSource:https://aws.amazon.com/vi/blogs/compute/implementing-advanced-aws-graviton-adoption-strategies-across-aws-regions/\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.3-lambda-basics/",
	"title": "Lambda Basics",
	"tags": [],
	"description": "",
	"content": "Objectives Create and test a Lambda Hello World (Node.js/Python), pass input parameters, and fine-tune configurations (memory, timeout, log).\nKey Steps Create a Node.js function\nRuntime: Node.js 18.x (example). Sample handler returns JSON { message: \u0026quot;hello\u0026quot; }. Test event with a name parameter, log the result in CloudWatch. Create a Python function\nRuntime: Python 3.12 (example). Handler reads event[\u0026quot;name\u0026quot;] and returns a customized response. Parameterization \u0026amp; Common Errors\nRead the event (query/body will be attached by API Gateway in a later step). Catch missing key errors, return appropriate status codes. Light Optimization\nAdjust Memory and Timeout to balance cost/performance. Set CloudWatch retention. "
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.4-api-gateway/5.4.3-test-api/",
	"title": "Test API Gateway",
	"tags": [],
	"description": "",
	"content": "Test GET Use curl:\ncurl \u0026quot;\u0026lt;invoke-url\u0026gt;/\u0026quot; Expected: HTTP 200, body {\u0026quot;message\u0026quot;:\u0026quot;Hello, tester\u0026quot;}. If the name is wrong or there\u0026rsquo;s a 5xx error, check CloudWatch logs. PS C:\\Users\\Nhan\\Documents\\hugo_aws\u0026gt; curl \u0026#34;https://tyuyo8aqwd.execute-api.us-east-1.amazonaws.com/dev/hello\u0026#34; StatusCode : 200 StatusDescription : OK Content : {\u0026#34;message\u0026#34;: \u0026#34;hello\u0026#34;} RawContent : HTTP/1.1 200 OK x-amzn-RequestId: 07039249-41a2-4024-8939-5b8d676f46e7 x-amz-apigw-id: VLGW2G0loAMEm5Q= X-Amzn-Trace-Id: Root=1-693450f8-a03e8d0a53d 6c1ca901a1626;Parent=5c411597312990c1;Sample d=0;L... Forms : {} Headers : {[x-amzn-RequestId, 07039249-41a2-4024-8939-5b8d676f46e7], [x-amz-apigw-id, VLGW2G0loAMEm5Q=], [X-Amzn-Trace-Id, Root=1-693450f8-a03e8d0a53 d6c1ca901a1626;Parent=5c411597312990c1;Sampl ed=0;Lineage=1:b8101292:0], [Connection, Keep-Alive]...} Images : {} InputFields : {} Links : {} ParsedHtml : System.__ComObject RawContentLength : 20 Test POST Use curl/Postman:\ncurl -X POST -H \u0026quot;Content-Type: application/json\u0026quot; -d '{\u0026quot;name\u0026quot;:\u0026quot;tester\u0026quot;}' \u0026quot;\u0026lt;invoke-url\u0026gt;/hello\u0026quot; Expected: JSON response with the correct name, status code 200. If Lambda doesn\u0026rsquo;t parse the body, check event.body and JSON.parse. C:\\Users\\Nhan\u0026gt;curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{}\u0026#39; \u0026#34;https://tyuyo8aqwd.execute-api.us-east-1.amazonaws.com/dev/hello\u0026#34; {\u0026#34;message\u0026#34;: \u0026#34;hello\u0026#34;} Error Handling (Optional) Return 400 when name is missing or body is not valid JSON. Log input for easier debugging: console.log(event) or log required fields. "
},
{
	"uri": "http://localhost:1313/hugo_aws/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "This section lists and introduces the blogs you have translated:\nBlog 1 – Strategies to excel in all four exam domains of the AWS Certified Machine Learning – Specialty certification. Blog 2 – From Theory to Practice Using Amazon Q Developer CLI to Create Customized AWS Projects Blog 3 – Deploying Advanced AWS Graviton Adoption Strategies Across AWS Regions "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.3-week3/",
	"title": "Week 3 - AWS Compute Services",
	"tags": [],
	"description": "",
	"content": "Week: 2025-09-22 to 2025-09-26\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 3 Overview This week highlighted AWS compute services, especially Amazon EC2 and supporting capabilities for scaling, storage, and pricing.\nKey Topics Amazon EC2 and instance families AMIs and backup strategies EBS volumes vs. Instance Store EC2 Auto Scaling patterns EC2 pricing models Amazon Lightsail, EFS, and FSx Hands-on Labs Lab 01: AWS Account \u0026amp; IAM Setup Lab 07: AWS Budgets \u0026amp; Cost Management Lab 09: AWS Support Plans Day 11-Amazon EC2 (Elastic Compute Cloud) provides scalable computing in the cloud – essentially Virtual Machines.\nAWS classifies instances into several groups to suit different workload needs. Main types:\nT (Burstable): E.g., T3, T4g. Best for applications with low CPU usage but occasional spikes (e.g., dev/test servers, low-traffic websites).\nM (General Purpose): E.g., M5, M6i. Balances CPU, Memory (RAM), and Network Resources. Suitable for most general-purpose applications.\nC (Compute Optimized): E.g., C5, C6i. Optimized for applications that require high CPU performance, such as high-performance computing (HPC), gaming servers, and video processing.\nR (Memory Optimized): Eg: R5, R6i. Optimized for applications that require large RAM, such as high-performance databases, in-memory Big Data analytics.\nI (Storage Optimized): Eg: I3, I4i. Optimized for tasks that require high-speed I/O and large local storage (e.g., NoSQL databases).\nDay 12-AMI and Backup Strategy\nAMI (Amazon Machine Image): Is a virtualization template (template) that contains the software configuration needed to launch an EC2 Instance. AMI includes the operating system, application server, and your application. AMI allows you to launch multiple identical Instances quickly.\nBackup Strategy: Use EBS Snapshots (see below) to create point-in-time backups of EBS volumes. This strategy should be automated (e.g., using AWS Backup or Amazon Data Lifecycle Manager) and stored off-site to ensure data is always recoverable.\nDay 13 - EC2 Auto Scaling (ASG) is a service that automatically adjusts the number of EC2 instances to meet traffic demands.\nFunction: Automatically launch additional instances when demand increases (Scale Out) and terminate instances when demand decreases (Scale In). Benefits: Ensure consistent application performance and optimize costs by paying only for the resources you actually need. Configuration: Define Minimum, Desired, and Maximum number of instances. Day 14-EC2 Pricing Model Selection\nOn-Demand Pay per hour/second, no upfront commitment. Short-term, infrequent, unpredictable workloads.\nSavings Plans Commit to a fixed spend (e.g., $10/hour) for 1 or 3 years to get a deep discount compared to On-Demand. Stable, persistent workloads.\nReserved Instances (RI) Commit to a specific Instance type for 1 or 3 years. Stable, persistent workloads (gradually replaced by Savings Plans).\nSpot Instances Bid on AWS\u0026rsquo;s excess compute capacity, which can offer discounts of up to 90%. AWS can reclaim Instances at any time. Non-critical, fault-tolerant workloads (e.g., batch processing, compute).\nDay 15-Other Compute and Storage Services\nAmazon Lightsail: Simple, fixed-price monthly provisioning of virtual servers (VPS), databases, and networking. Ideal for new users or small, simple projects. Amazon EFS (Elastic File System): Scalable, on-demand file storage service (Elastic), accessed concurrently by multiple EC2 Instances across multiple AZs. (Using NFS protocol). Amazon FSx: Provides fully managed third-party file systems on AWS. "
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.4-api-gateway/",
	"title": "API Gateway Integration",
	"tags": [],
	"description": "",
	"content": "Goal Publish Lambda through API Gateway (REST), create resources/methods, and test GET/POST with curl or Postman.\nSteps Create REST API: new API, stage to a named environment (e.g., dev). Resource \u0026amp; methods: create /hello, add GET and/or POST, choose Lambda integration, select your function. Deploy stage: deploy to dev and note the invoke URL. Test: GET: curl \u0026quot;\u0026lt;invoke-url\u0026gt;/hello?name=alice\u0026quot; POST: curl -X POST -H \u0026quot;Content-Type: application/json\u0026quot; -d '{\u0026quot;name\u0026quot;:\u0026quot;alice\u0026quot;}' \u0026quot;\u0026lt;invoke-url\u0026gt;/hello\u0026quot; Handle input in Lambda: read event.queryStringParameters for GET, parse event.body for POST; return JSON with proper status codes. "
},
{
	"uri": "http://localhost:1313/hugo_aws/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "Throughout my internship, I had the opportunity to participate in several impactful events that enriched my professional journey with valuable insights and memorable experiences.\nEvent 1 Event Name: Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: 09:00 – 17:00 VNT, Thursday, September 18, 2025\nLocation: 36th Floor, 2 Hai Trieu Street, Sai Gon Ward, Ho Chi Minh City\nRole: Attendee\nDescription: Vietnam Cloud Day 2025 was a comprehensive AWS event featuring keynote addresses from government speakers, AWS executives, and industry leaders. The event included two main tracks: a live telecast track with keynotes and panel discussions on GenAI revolution and executive leadership, and breakout tracks covering topics such as unified data foundations for AI/analytics, GenAI adoption roadmaps, AI-driven development lifecycle, securing GenAI applications, and AI agents for productivity. The event showcased AWS\u0026rsquo;s latest services and strategic initiatives for AI and cloud modernization.\nOutcomes: Gained insights into enterprise-scale AI adoption strategies, learned about AWS services for data foundation and GenAI implementation, and understood best practices for securing AI applications and modernizing legacy systems.\nEvent 2 Event Name: AWS GenAI Builder Club - AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 14:00 (2:00 PM), Friday, October 3, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, Ho Chi Minh City\nRole: Attendee\nDescription: This AWS GenAI Builder Club session focused on the AI-Driven Development Lifecycle (AI-DLC), exploring how generative AI transforms software development from architecture through deployment and maintenance. The session featured demonstrations of Amazon Q Developer and Kiro, showcasing how AI can automate undifferentiated heavy lifting tasks and enable developers to focus on higher-value, creative work. The agenda included an overview of AI-DLC concepts, Amazon Q Developer capabilities, and hands-on Kiro demonstrations.\nOutcomes: Learned practical applications of AI in the software development lifecycle, gained hands-on experience with Amazon Q Developer and Kiro tools, and understood how to integrate AI as a central collaborator in development processes to increase productivity and code quality.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.4-api-gateway/5.4.4-cors-custom-domain/",
	"title": "Optional: CORS &amp; Custom Domain",
	"tags": [],
	"description": "",
	"content": "Enable CORS Select the method → Enable CORS → add the Content-Type header and required methods. Redeploy the stage after enabling CORS. Custom Domain (Optional) API Gateway → Custom domain names → create a domain and map the dev stage → /. Update DNS CNAME to point to the API Gateway domain. Retest Call GET/POST again through the new domain or with appropriate CORS headers. Check logs if the browser blocks the request due to CORS. "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.4-week4/",
	"title": "Week 4 - AWS Storage Services",
	"tags": [],
	"description": "",
	"content": "Week: 2025-09-29 to 2025-10-03\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 4 Overview This week deep-dived into AWS storage services, ranging from S3 object storage to hybrid storage integrations.\nKey Topics Amazon S3 and storage classes S3 static website hosting S3 Glacier for archival workloads AWS Snow Family AWS Storage Gateway Disaster Recovery strategies AWS Backup Hands-on Labs Lab 13: AWS Backup Lab 14: AWS VM Import/Export Lab 24: AWS Storage Gateway Lab 25: Amazon FSx Lab 57: Amazon S3 \u0026amp; CloudFront Day 16\nAmazon S3 is an Object Storage service that provides unlimited scalability, high durability, and high availability. Data is stored as Objects inside Buckets.\nDay 17-S3 Static Website Hosting\nFunction: S3 can be used to host static websites (HTML, CSS, JavaScript, images) without the need for EC2 web servers.\nBenefits: Extremely simple, low cost, and the ability to automatically scale to global traffic demands.\nDay 18-S3 Glacier are storage classes specifically designed for long-term data storage (Archival), where low cost is the top priority and acceptable retrieval times are minutes or hours.\nDay 19-AWS Snow Family\nA physical service for moving large amounts of data into or out of AWS, especially when the Internet is low bandwidth or high cost.\nSnowcone: The smallest mobile device, used for data collection, processing (Edge Compute) and data transfer.\nSnowball Edge: A suitcase-sized device, used to move Petabytes of data and support Edge Computing.\nSnowmobile: An Exabyte-scale data truck for the largest data movements.\nDay 20-AWS Storage Gateway\nA Hybrid Cloud service that allows you to connect your On-premises infrastructure to AWS cloud storage.\nOperational: Install a virtual machine (VM) on-premises, which acts as a gateway to seamlessly store data to S3 or EBS.\nMain Gateway types: File Gateway (file sharing via SMB/NFS), Volume Gateway (virtual block storage), and Tape Gateway (replaces physical tape library).\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.5-sample-app/",
	"title": "Sample App Logic",
	"tags": [],
	"description": "",
	"content": "Exercise: In-App Purchase Suggestions Build a Lambda function that receives a list of purchased items and returns suggestions for items not yet purchased (simple set subtraction).\nInput Requirements { \u0026#34;allItems\u0026#34;: [\u0026#34;starter_pack\u0026#34;,\u0026#34;booster_pack\u0026#34;,\u0026#34;data_pack\u0026#34;,\u0026#34;golden_apples\u0026#34;,\u0026#34;skins\u0026#34;], \u0026#34;owned\u0026#34;: [\u0026#34;data_pack\u0026#34;,\u0026#34;starter_pack\u0026#34;] } Processing Use set operations to get the remaining items: suggestions = allItems - owned. Return JSON { \u0026quot;suggestions\u0026quot;: [...] } with status code 200. Log input/output for easier debugging. Testing In the console: create a test event with various owned values. Via API Gateway: POST /suggest with JSON body as shown above. Test error cases: missing allItems or owned → return 400 with a clear message. "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.5-week5/",
	"title": "Week 5 - AWS Security &amp; Identity",
	"tags": [],
	"description": "",
	"content": "Week: 2025-10-06 to 2025-10-10\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 5 Overview This week concentrated on AWS security and identity management foundations.\nKey Topics Shared Responsibility Model AWS IAM (Users, Groups, Roles, Policies) Amazon Cognito AWS Organizations \u0026amp; SCPs AWS Identity Center (SSO) AWS KMS AWS Security Hub Hands-on Labs Lab 18: AWS Security Hub Lab 22: AWS Lambda Automation with Slack Lab 27: AWS Resource Groups \u0026amp; Tagging Lab 28: IAM Cross-Region Role \u0026amp; Policy Lab 30: IAM Restriction Policy Lab 33: AWS KMS \u0026amp; CloudTrail Integration Lab 44: IAM Advanced Role Control Lab 48: IAM Access Keys \u0026amp; Roles Day 21-AWS IAM (Identity and Access Management)\nA service that allows you to securely manage access to AWS services and resources.\nUsers: Represents a specific person or application. Each User can have individual credentials (Console and/or Access Keys). Groups: A collection of Users. Assign permissions (Policies) to a Group to apply to all Users in it, making management easier. Roles: A collection of access permissions that you can assign to AWS entities (such as EC2 Instances, Lambda Functions) or external users/federated logins (Federated Users). Roles are the preferred mechanism for providing secure, temporary permissions. Policies: A document (in JSON format) that defines who (Principal) is allowed to perform what Action on what Resource (Resource) and under what Conditions (Condition). Day 22-Amazon Cognito\nA service that helps manage user identity and authentication for web and mobile applications.\nUser Pools: Provides user registration, login, and profile management for your applications. Supports multi-factor authentication (MFA). Identity Pools: Allows application users to access AWS services (e.g., S3, DynamoDB) by exchanging Cognito credentials with temporary IAM Roles. Day 23- AWS Organizations \u0026amp; SCPs\nAWS Organizations: A centralized management service that allows you to aggregate and manage multiple AWS accounts under a single organization. Helps optimize billing, security, and compliance.\nSCPs (Service Control Policies): Policies applied at the Organization (or Organizational Unit - OU) level to set maximum permission limits for all Users and Roles in the accounts under that OU. SCPs are a secure \u0026ldquo;filter\u0026rdquo; that cannot be bypassed.\nDay 24- AWS Identity Center (SSO)\n-AWS Identity Center (formerly AWS Single Sign-On - SSO): Single sign-on (SSO) access management service so users can centrally access all their AWS accounts and other cloud applications (e.g., Office 365, Salesforce) with just one set of credentials.\nDay 25-AWS KMS (Key Management Service)\nFunction: Managed service to create and control encryption keys (Encryption Keys) used to encrypt data.\nHighly Secure: KMS is designed so that no one, not even AWS, can extract the master KMS keys (CMKs - Customer Master Keys) in clear text. It integrates deeply with most AWS services (S3, EBS, RDS, etc.).\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Serverless Workshop: AWS Lambda \u0026amp; API Gateway Overview Hands-on workshop introducing serverless fundamentals, AWS Lambda programming model, and exposing functions via API Gateway. Participants build Hello World functions (Node.js/Python), add parameters, and deploy a sample in-app purchase suggestion API. Focus on delivering quickly without server management, cost awareness, and solid operational practices.\nAgenda Serverless foundations: why Lambda, when to choose it, regional considerations. Hello World fast path: create/test Lambda from console (Node.js/Python); adjust memory/timeouts. Handling inputs: events, query strings, request body parsing, common error patterns. API Gateway integration: create resources/methods, map to Lambda, deploy stages, test via curl/Postman. Sample app: in-app purchase recommender (purchased vs. available items). Cost \u0026amp; ops: memory tuning, cold-start awareness, logging, basic error handling. Next steps: custom runtimes, serverless frameworks, edge deployments. What You’ll Build Two Lambda functions (Node.js and Python) with parameterized responses. One Lambda-backed REST endpoint via API Gateway (GET/POST). A simple business-logic function (in-app purchase suggestions) returning JSON. Prerequisites AWS account with console access. Basic familiarity with JavaScript or Python. curl or Postman for API testing. References Console examples: Node.js \u0026amp; Python Hello World API Gateway setup steps Sample in-app purchase logic Cleanup checklist "
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Cleanup Steps Delete the API Gateway stage/API created for the workshop.\nDelete the Lambda functions (Node.js, Python, suggest).\nRemove IAM roles/policies created specifically for Lambda (if no longer needed). Check CloudWatch Log Groups, set short retention or delete. Ensure no remaining resources that incur costs (temporary S3 buckets, temporary Secrets if any). Cost Notes Lambda/Logs/API Gateway have low costs but should still be deleted after the lab. If you created additional resources (S3, KMS), confirm they have been deleted/retention set. "
},
{
	"uri": "http://localhost:1313/hugo_aws/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at First Cloud Journey (FCJ) - AWS from September 8, 2025 to November 28, 2025 (12 weeks), I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in multiple AWS cloud computing learning tracks and hands-on projects, through which I improved my skills in cloud architecture design, AWS services implementation, technical documentation, blog translation, and workshop development.\nKey Accomplishments Throughout the internship, I completed the following major tasks:\nAWS Learning Journey (12 Weeks): Systematically studied core AWS services including EC2, S3, RDS, Lambda, API Gateway, VPC, IAM, CloudWatch, and security best practices.\nChallenge Proposal: Developed comprehensive proposals for cloud infrastructure challenges and solutions.\nBlog Translation: Translated 3+ technical AWS blog posts from English to Vietnamese, covering topics such as Amazon SageMaker, EC2 Mac instances, and AMD optimization guides.\nWorkshop Development: Created a hands-on workshop on AWS services integration with practical examples and step-by-step tutorials for hands-on labs.\nEvent Participation: Attended 2 major AWS events:\nVietnam Cloud Day 2025 (September 18, 2025) AWS GenAI Builder Club - AI-Driven Development Life Cycle (October 3, 2025) In terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality X 2 Ability to learn Ability to absorb new knowledge and learn quickly X 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions X 4 Sense of responsibility Completing tasks on time and ensuring quality X 5 Discipline Adhering to schedules, rules, and work processes X 6 Progressive mindset Willingness to receive feedback and improve oneself X 7 Communication Presenting ideas and reporting work clearly X 8 Teamwork Working effectively with colleagues and participating in teams X 9 Professional conduct Respecting colleagues, partners, and the work environment X 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity X 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team X 12 Overall General evaluation of the entire internship period X Strengths Strong technical foundation: Successfully learned and applied a wide range of AWS services across compute, storage, networking, database, and security domains. Documentation skills: Produced high-quality technical documentation including proposals, translated blogs, and workshop materials. Teamwork \u0026amp; collaboration: Actively participated in team activities and AWS events, networking with FCJ members and industry professionals. Self-learning ability: Demonstrated capability to independently study complex cloud concepts and implement hands-on labs. Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking and approach challenges more systematically Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively Better time management to meet deadlines more consistently "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.6-week6/",
	"title": "Week 6 - AWS Database Services",
	"tags": [],
	"description": "",
	"content": "Week: 2025-10-13 to 2025-10-17\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 6 Overview This week focused on the AWS database landscape, covering managed relational engines, purpose-built NoSQL stores, in-memory caching, and analytics data warehouses.\nKey Topics Database Fundamentals (RDBMS, NoSQL, OLTP vs OLAP) Amazon RDS \u0026amp; Aurora Amazon Redshift Amazon ElastiCache AWS Database Migration Service (DMS) Hands-on Labs Lab 05: Amazon RDS \u0026amp; EC2 Integration Lab 43: AWS Database Migration Service (DMS) Day 26-Basic Concepts of Databases\nRDBMS (Relational Database Management System) Relational database (e.g. MySQL, PostgreSQL). Data is organized into tables, with constraints and transactions (ACID). NoSQL (Not Only SQL) Non-relational database (e.g. DynamoDB, MongoDB). Data is flexible, no need for a rigid schema. OLTP (Online Transaction Processing) Online transaction processing. Focus on small, fast transactions (INSERT, UPDATE, DELETE). OLAP (Online Analytical Processing) Online analytical processing. Focus on complex queries, synthesizing large amounts of data. Day 27-Amazon Redshift\nNature: Cloud-based Data Warehouse service, optimized for OLAP (analytics). Function: Enables complex queries and data analysis on a Petabyte scale, using Columnar Storage architecture and Massively Parallel Processing (MPP) to speed up queries. Day 28- Amazon ElastiCache\nNature: Fully managed In-Memory Caching service. Function: Helps speed up applications by offloading the database and storing frequent queries or session data in RAM. Supported Engines:Redis: Supports rich, durable data structures, replication and queuing. Memcached: Simple, high-speed, often used for object caching. Day 29-Amazon RDS \u0026amp; Aurora\nAmazon RDS (Relational Database Service) is a service that manages traditional relational databases on AWS. It handles patching, backups, fault management, and scaling.\nSupported Engines: PostgreSQL, MySQL, MariaDB, Oracle, SQL Server. Amazon Aurora: Is an AWS-exclusive relational DB engine, compatible with MySQL and PostgreSQL. Aurora provides higher performance (5x MySQL, 3x PostgreSQL) and higher availability at a lower cost than commercial DBs. Day 30-AWS Database Migration Service (DMS)\nFunction: A tool that allows you to migrate databases (both relational and NoSQL) from On-premises to AWS, between AWS services, or between different engine types (e.g., from Oracle to Aurora PostgreSQL). Key Features: Supports Continuous Replication (Zero Downtime Migration) to minimize downtime during conversion. "
},
{
	"uri": "http://localhost:1313/hugo_aws/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.7-week7/",
	"title": "Week 7 - Vertical Slice Delivery",
	"tags": [],
	"description": "",
	"content": "Week: 2025-10-20 to 2025-10-24\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 7 Overview This week delivered vertical slice 0 for the Ebook Demo project, emphasizing contract-first development and automated testing to enable an end-to-end demo.\nKey Topics Vertical Slice Architecture and the scope of slice 0 Contract-first development with OpenAPI + Prism mocks Next.js 16 App Router \u0026amp; Server Components FastAPI clean architecture and CORS configuration Schemathesis contract testing and retrospective learnings Hands-on Labs Demo checklist for vertical slice 0 Mock API with Prism and connect it to Next.js Refactor FastAPI backend following clean architecture Run Schemathesis and update the validated workflow "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.8-week8/",
	"title": "Week 8 - Natural Language Processing &amp; Deep Learning",
	"tags": [],
	"description": "",
	"content": "Week: 2025-10-27 to 2025-10-31\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 8 Overview This week provides a comprehensive deep-dive into Natural Language Processing (NLP), covering linguistic foundations, modern NLP applications, sequence-to-sequence architectures, and evaluation methodologies. From understanding phonetics to implementing machine translation systems, this week bridges theory and practice in NLP.\nKey Topics Linguistic Foundations Core components: Phonetics, Phonology, Morphology, Syntax, Semantics, Pragmatics Understanding how language structure informs NLP design NLP Applications Search engines and intent recognition Online advertising with NER and relationship extraction Voice assistants and speech recognition Chatbots with NLU/NLG pipelines Machine translation systems Text summarization (extractive \u0026amp; abstractive) Deep Learning Architectures Seq2seq models with encoder-decoder architecture LSTM deep dive: forget gate, input gate, cell state, output gate Attention mechanism and self-attention Neural Machine Translation (NMT) implementation Evaluation \u0026amp; Decoding BLEU score (precision-based) ROUGE score (recall-based) F1 score for MT evaluation Beam search decoding Minimum Bayes Risk (MBR) sampling Hands-on Labs Building voicebot and chatbot workflows Implementing LSTM for sequence modeling Creating encoder-decoder with attention Neural machine translation end-to-end Evaluating translation quality with BLEU/ROUGE Implementing beam search and MBR "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.9-week9/",
	"title": "Week 9 - Transformer Architecture &amp; Implementation",
	"tags": [],
	"description": "",
	"content": "Week: 2025-11-03 to 2025-11-07\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 9 Overview This week explores the Transformer architecture, a revolutionary model that replaced RNNs in NLP. We\u0026rsquo;ll understand why transformers are needed, how they work internally, and implement them from scratch. From attention mechanisms to the full encoder-decoder design, this week bridges theory and practical implementation.\nKey Topics RNN Limitations \u0026amp; Transformer Introduction Sequential processing bottlenecks in RNNs Vanishing gradient problems Information bottleneck with long sequences Why attention is all you need Transformer Architecture Encoder-decoder structure Multi-head attention layers Positional encoding Residual connections \u0026amp; layer normalization Feed-forward networks Attention Mechanisms Scale dot-product attention (core mechanism) Self-attention (same sentence) Masked attention (decoder) Encoder-decoder attention Multi-head attention for parallel computation Transformer Decoder \u0026amp; GPT2 Positional embeddings Decoder block implementation Feed-forward layer design Output probability calculation Applications \u0026amp; Models GPT-2 (Generative Pre-trained Transformer) BERT (Bidirectional Encoder Representations) T5 (Text-to-Text Transfer Transformer) Applications: Translation, Classification, QA, Summarization, Sentiment Analysis Learning Objectives ✅ Understand RNN limitations and why transformers solve them ✅ Grasp the complete transformer architecture ✅ Implement attention mechanisms from scratch ✅ Build a transformer decoder (GPT2-style) ✅ Recognize transformer applications and state-of-the-art models Daily Breakdown Day Focus Topics 41 RNN Problems Sequential processing, Vanishing gradients, Information bottleneck 42 Architecture Overview Encoder-decoder, Multi-head attention, Positional encoding 43 Attention Core Scale dot-product attention formula, Matrix operations, GPU efficiency 44 Attention Types Self-attention, Masked attention, Encoder-decoder attention 45 Decoder Implementation GPT2 architecture, Building blocks, Code walkthrough Prerequisites Deep understanding of RNNs, LSTMs, and attention from Week 8 Comfortable with matrix operations and linear algebra PyTorch or TensorFlow knowledge helpful Next Steps Study the paper \u0026ldquo;Attention is All You Need\u0026rdquo; (Vaswani et al., 2017) Implement transformer components incrementally Experiment with pre-trained models (BERT, GPT-2, T5) "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.10-week10/",
	"title": "Week 10 - Transfer Learning, BERT &amp; T5",
	"tags": [],
	"description": "",
	"content": "Week: 2025-11-10 to 2025-11-14\nStatus: \u0026ldquo;In progress\u0026rdquo;\nWeek 10 Overview This week dives into transfer learning for NLP and how modern QA systems benefit from pre-trained transformers. We will contrast classical training with feature-based reuse and fine-tuning, then study two flagship models: BERT for bidirectional context and T5 for text-to-text multitask learning, along with practical QA setups (context-based vs. closed-book).\nKey Topics Transfer Learning Fundamentals Classical vs. transfer learning pipelines Reusing pre-trained weights to speed convergence Feature-based representations vs. fine-tuning Benefits: faster training, better predictions, less labeled data Question Answering Modes Context-based QA (span extraction with provided context) Closed-book QA (generate answers without context) How pre-training quality shapes QA performance BERT Bidirectional Context Masked language modeling for contextual embeddings Next sentence prediction for sentence-level coherence Using both left and right context to predict tokens Typical downstream uses: QA, sentiment, classification T5 Text-to-Text Multitask Unified text-to-text framing for multiple tasks Prompting the same model for rating, QA, summarization, translation Scaling with large corpora (e.g., C4 vs. Wikipedia) Multitask transfer to improve generalization Training \u0026amp; Data Strategy Labeled vs. unlabeled data mix; self-supervised masking Freezing backbone vs. adding task heads Fine-tuning recipes for downstream tasks (QA, summarization, translation) Learning Objectives ✅ Explain transfer learning and when to prefer it over training from scratch ✅ Distinguish feature-based reuse from full fine-tuning ✅ Compare context-based QA and closed-book QA setups ✅ Summarize how BERT and T5 pre-train and transfer across tasks ✅ Identify why transfer learning reduces data needs and training time Daily Breakdown Day Focus Topics 46 Transfer Learning Intro Classical vs. transfer pipeline, reuse weights, feature-based vs. fine-tuning, benefits 47 Question Answering Context-based span QA vs. closed-book QA, data needs, evaluation cues 48 BERT Bidirectionality Masked LM, next sentence prediction, leveraging both contexts for token prediction 49 T5 Multitask Model Text-to-text prompts, multitask sharing, scaling data (C4 vs. Wikipedia) 50 Fine-tuning Practice Freezing layers vs. adding heads, downstream tasks: QA, summarization, translation Prerequisites Solid grasp of transformer architecture from Week 9 Comfortable with attention mechanisms and encoder-decoder flow Basic familiarity with PyTorch or TensorFlow for fine-tuning Next Steps Read the BERT and T5 papers to internalize pre-training objectives Fine-tune a pre-trained BERT QA model (e.g., SQuAD-style span extraction) Experiment with T5 prompts for QA, summarization, and sentiment tasks Compare feature-based vs. fine-tuned performance on your own dataset "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.11-week11/",
	"title": "Week 11 - Lambda Managed Instances &amp; AWS re:Invent Learnings",
	"tags": [],
	"description": "",
	"content": "Week: 2025-11-17 to 2025-11-21\nStatus: \u0026ldquo;Planned\u0026rdquo;\nWeek 11 Overview Content from AWS re:Invent 2025 (session CNS382) on Lambda Managed Instances (LMI): running Lambda functions on EC2 capacity with full serverless programming model, better control over compute, and reuse of EC2 pricing models. Focus on when to choose LMI vs. default Lambda, how to configure capacity providers, and how testing/ops practices change for predictable high-traffic workloads.\nKey Topics Why Lambda Managed Instances Keep Lambda dev experience while choosing EC2 instance families and pricing constructs Eliminate cold starts; support multi-concurrency per instance Apply EC2 Savings Plans/Reserved Instances; predictability for steady traffic Architecture \u0026amp; Setup Capacity Provider: VPC config, instance types (C/M/R families, x86/Graviton), scaling guardrails Steps: create capacity provider -\u0026gt; create function bound to provider -\u0026gt; publish version to launch instances LMI-managed lifecycle: AWS patches OS/runtime, handles routing/auto scaling; instances visible but immutable Networking \u0026amp; Security All egress via instance ENI in provider VPC; function-level VPC config disabled Close inbound SG rules; ensure paths to dependencies/CloudWatch (Internet or PrivateLink) EBS encryption (service key or custom KMS) Function Features \u0026amp; Scaling Supports ZIP/OCI packaging; Java/Python/Node/.NET runtimes; layers, extensions, function URLs, response streaming, durable functions Memory/CPU settings influence instance choice; optional overrides for allowed/excluded instance types Instance-level scaling guardrails (e.g., max vCPU) to control cost Workload Fit \u0026amp; Trade-offs Use LMI for high-traffic steady workloads, specialized compute/memory/network needs Keep default Lambda for spiky/short, unpredictable invocations Multi-concurrency and EC2 billing change cost/perf envelope Learning Objectives Explain when to prefer LMI vs. default Lambda for serverless apps Configure capacity providers with VPC, role, and instance constraints Describe LMI networking model and logging path Map Lambda features (packaging, runtimes, URLs, streaming, durable) to LMI Set scaling/cost guardrails and choose instance families for workload shapes Daily Breakdown Day Focus Topics 51 LMI Overview \u0026amp; Use Cases Why LMI, benefits, EC2 pricing reuse, when not to use it 52 Capacity Provider Setup VPC config, IAM operator role, instance families, KMS, guardrails 53 Functions on LMI Packaging/runtime support, memory/CPU mapping, multi-concurrency, publishing versions 54 Networking \u0026amp; Observability Egress paths, CloudWatch access, security groups, logging, monitoring notes 55 Scaling \u0026amp; Ops Playbook Max vCPU, steady traffic planning, cost controls, rollout checklist Prerequisites Familiarity with Lambda programming model and transformer material from prior weeks Basic EC2/VPC, IAM roles/policies, and CloudWatch logging Understanding of Savings Plans/Reserved Instances for EC2 Next Steps Draft a capacity provider for your target workload (VPC, instance shortlist, guardrails) Plan benchmarks comparing LMI vs. default Lambda on representative traffic Map monitoring and logging paths (CloudWatch endpoints, PrivateLink if needed) Decide pricing instruments (SP/RI) and multi-concurrency targets per function "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.12-week12/",
	"title": "Week 12 - AWS re:Invent 2025 Announcements",
	"tags": [],
	"description": "",
	"content": "Week: 2025-11-24 to 2025-11-28\nStatus: \u0026ldquo;Planned\u0026rdquo;\nWeek 12 Overview Highlights from AWS re:Invent 2025: new Nova foundation models (speech-to-speech, multimodal, cost-effective reasoning), Bedrock expansions (open-weight models, reinforcement fine-tuning), vector and AI infra updates (S3 Vectors GA), and compute launches like Graviton5 CPUs and Trainium3 UltraServers. Focus on mapping announcements to practical adoption plans.\nKey Topics GenAI \u0026amp; Models Nova 2 family: Sonic (speech-to-speech), Lite (fast/cost-efficient), Omni (multimodal), Forge program for custom frontier models Nova Act for reliable UI agents; Bedrock AgentCore adds policy controls and quality evals Bedrock adds open-weight models (Mistral Large 3, Ministral 3) and reinforcement fine-tuning Vector \u0026amp; Data Amazon S3 Vectors GA: up to 2B vectors/index, ~100ms queries, lower cost vs. specialty DBs Clean Rooms synthetic data for privacy-preserving ML collaboration AI Dev Platform SageMaker AI serverless MLflow and new training features (checkpointless, elastic scaling) Compute \u0026amp; Hardware Graviton5 CPUs for better price/perf on EC2 Trainium3 UltraServers (3nm) for faster, cheaper training/inference Learning Objectives Identify which re:Invent AI/compute launches impact current workloads Plan pilot use cases for Nova models and Bedrock new capabilities Outline migration path to S3 Vectors for vector search storage Evaluate Graviton5/Trainium3 fit for cost/performance gains Daily Breakdown Day Focus Topics 56 Model Announcements Nova 2 variants (Sonic, Lite, Omni), Forge program, Nova Act agents 57 Bedrock \u0026amp; Agents Open-weight additions, reinforcement fine-tuning, AgentCore policy/quality 58 Vector \u0026amp; Data S3 Vectors GA, Clean Rooms synthetic data, vector scale/cost planning 59 SageMaker Platform Serverless MLflow, checkpointless \u0026amp; elastic training on HyperPod 60 Compute Launches Graviton5 CPUs, Trainium3 UltraServers, workload fit \u0026amp; migration checklist Prerequisites Familiarity with Bedrock model catalog and agent capabilities Basics of vector search architectures Understanding of EC2 instance families and accelerator choices Next Steps Select one pilot use case for Nova (speech, multimodal, or reasoning) and outline eval plan Draft migration/POC plan for S3 Vectors vs. current vector store Benchmark targets for Graviton5/Trainium3 against existing instances Define governance/policy needs before adopting AgentCore and Nova Act agents "
},
{
	"uri": "http://localhost:1313/hugo_aws/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/hugo_aws/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]